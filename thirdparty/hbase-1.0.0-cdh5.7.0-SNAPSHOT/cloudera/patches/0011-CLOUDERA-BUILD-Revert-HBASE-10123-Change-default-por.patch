From 4223e6582601e23aa17219c379d66809793c3359 Mon Sep 17 00:00:00 2001
From: Matteo Bertozzi <matteo.bertozzi@cloudera.com>
Date: Thu, 18 Dec 2014 15:36:30 +0000
Subject: [PATCH 011/226] CLOUDERA-BUILD Revert HBASE-10123 Change default ports; move them out of linux ephemeral port range

---
 bin/local-master-backup.sh                         |    4 +-
 .../hadoop/hbase/client/TestClientNoCluster.java   |    4 +-
 .../apache/hadoop/hbase/HBaseConfiguration.java    |    2 +-
 .../java/org/apache/hadoop/hbase/HConstants.java   |   10 +-
 hbase-common/src/main/resources/hbase-default.xml  |    6 +-
 .../org/apache/hadoop/hbase/LocalHBaseCluster.java |    6 +-
 src/main/docbkx/configuration.xml                  | 1653 +++++++++++++++++++
 src/main/docbkx/troubleshooting.xml                | 1700 ++++++++++++++++++++
 8 files changed, 3369 insertions(+), 16 deletions(-)
 create mode 100644 src/main/docbkx/configuration.xml
 create mode 100644 src/main/docbkx/troubleshooting.xml

diff --git a/bin/local-master-backup.sh b/bin/local-master-backup.sh
index 36a70fa..ea1e528 100755
--- a/bin/local-master-backup.sh
+++ b/bin/local-master-backup.sh
@@ -42,8 +42,8 @@ run_master () {
   DN=$2
   export HBASE_IDENT_STRING="$USER-$DN"
   HBASE_MASTER_ARGS="\
-    -D hbase.master.info.port=`expr 16010 + $DN` \
-    -D hbase.regionserver.port=`expr 16020 + $DN` \
+    -D hbase.master.info.port=`expr 60000 + $DN` \
+    -D hbase.regionserver.port=`expr 60010 + $DN` \
     -D hbase.regionserver.info.port=`expr 16030 + $DN` \
     --backup"
   "$bin"/hbase-daemon.sh --config "${HBASE_CONF_DIR}" $1 master $HBASE_MASTER_ARGS
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
index 2d50c1b..fccf7fc 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
@@ -96,7 +96,7 @@ public class TestClientNoCluster extends Configured implements Tool {
   private static final Log LOG = LogFactory.getLog(TestClientNoCluster.class);
   private Configuration conf;
   public static final ServerName META_SERVERNAME =
-      ServerName.valueOf("meta.example.org", 16010, 12345);
+      ServerName.valueOf("meta.example.org", 60010, 12345);
 
   @Before
   public void setUp() throws Exception {
@@ -661,7 +661,7 @@ public class TestClientNoCluster extends Configured implements Tool {
   private static ServerName [] makeServerNames(final int count) {
     ServerName [] sns = new ServerName[count];
     for (int i = 0; i < count; i++) {
-      sns[i] = ServerName.valueOf("" + i + ".example.org", 16010, i);
+      sns[i] = ServerName.valueOf("" + i + ".example.org", 60010, i);
     }
     return sns;
   }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
index 1323240..fdb5c53 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
@@ -215,7 +215,7 @@ public class HBaseConfiguration extends Configuration {
 
   /** For debugging.  Dump configurations to system output as xml format.
    * Master and RS configurations can also be dumped using
-   * http services. e.g. "curl http://master:16010/dump"
+   * http services. e.g. "curl http://master:60010/dump"
    */
   public static void main(String[] args) throws Exception {
     HBaseConfiguration.create().writeXml(System.out);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
index b50b6d5..13cf01c 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -140,10 +140,10 @@ public final class HConstants {
   public static final String MASTER_PORT = "hbase.master.port";
 
   /** default port that the master listens on */
-  public static final int DEFAULT_MASTER_PORT = 16000;
+  public static final int DEFAULT_MASTER_PORT = 60000;
 
   /** default port for master web api */
-  public static final int DEFAULT_MASTER_INFOPORT = 16010;
+  public static final int DEFAULT_MASTER_INFOPORT = 60010;
 
   /** Configuration key for master web API port */
   public static final String MASTER_INFO_PORT = "hbase.master.info.port";
@@ -218,10 +218,10 @@ public final class HConstants {
   public static final String REGIONSERVER_PORT = "hbase.regionserver.port";
 
   /** Default port region server listens on. */
-  public static final int DEFAULT_REGIONSERVER_PORT = 16020;
+  public static final int DEFAULT_REGIONSERVER_PORT = 60020;
 
   /** default port for region server web api */
-  public static final int DEFAULT_REGIONSERVER_INFOPORT = 16030;
+  public static final int DEFAULT_REGIONSERVER_INFOPORT = 60030;
 
   /** A configuration key for regionserver info port */
   public static final String REGIONSERVER_INFO_PORT =
@@ -1013,7 +1013,7 @@ public final class HConstants {
    * The port to use for the multicast messages.
    */
   public static final String STATUS_MULTICAST_PORT = "hbase.status.multicast.address.port";
-  public static final int DEFAULT_STATUS_MULTICAST_PORT = 16100;
+  public static final int DEFAULT_STATUS_MULTICAST_PORT = 60100;
 
   public static final long NO_NONCE = 0;
 
diff --git a/hbase-common/src/main/resources/hbase-default.xml b/hbase-common/src/main/resources/hbase-default.xml
index c0f3f9a..007f8e1 100644
--- a/hbase-common/src/main/resources/hbase-default.xml
+++ b/hbase-common/src/main/resources/hbase-default.xml
@@ -98,7 +98,7 @@ possible configurations would overwhelm and obscure the important.
   <!--Master configurations-->
   <property>
     <name>hbase.master.info.port</name>
-    <value>16010</value>
+    <value>60010</value>
     <description>The port for the HBase Master web UI.
     Set to -1 if you do not want a UI instance run.</description>
   </property>
@@ -152,12 +152,12 @@ possible configurations would overwhelm and obscure the important.
   <!--RegionServer configurations-->
   <property>
     <name>hbase.regionserver.port</name>
-    <value>16020</value>
+    <value>60020</value>
     <description>The port the HBase RegionServer binds to.</description>
   </property>
   <property>
     <name>hbase.regionserver.info.port</name>
-    <value>16030</value>
+    <value>60030</value>
     <description>The port for the HBase RegionServer web UI
     Set to -1 if you do not want the RegionServer UI to run.</description>
   </property>
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
index f1fd7d2..832c721 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
@@ -48,12 +48,12 @@ import org.apache.hadoop.hbase.util.JVMClusterUtil;
  * to close it all down. {@link #join} the cluster is you want to wait on
  * shutdown completion.
  *
- * <p>Runs master on port 16000 by default.  Because we can't just kill the
+ * <p>Runs master on port 60000 by default.  Because we can't just kill the
  * process -- not till HADOOP-1700 gets fixed and even then.... -- we need to
  * be able to find the master with a remote client to run shutdown.  To use a
- * port other than 16000, set the hbase.master to a value of 'local:PORT':
+ * port other than 60000, set the hbase.master to a value of 'local:PORT':
  * that is 'local', not 'localhost', and the port number the master should use
- * instead of 16000.
+ * instead of 60000.
  *
  */
 @InterfaceAudience.Public
diff --git a/src/main/docbkx/configuration.xml b/src/main/docbkx/configuration.xml
new file mode 100644
index 0000000..3ad8326
--- /dev/null
+++ b/src/main/docbkx/configuration.xml
@@ -0,0 +1,1653 @@
+<?xml version="1.0"?>
+<chapter
+  xml:id="configuration"
+  version="5.0"
+  xmlns="http://docbook.org/ns/docbook"
+  xmlns:xlink="http://www.w3.org/1999/xlink"
+  xmlns:xi="http://www.w3.org/2001/XInclude"
+  xmlns:svg="http://www.w3.org/2000/svg"
+  xmlns:m="http://www.w3.org/1998/Math/MathML"
+  xmlns:html="http://www.w3.org/1999/xhtml"
+  xmlns:db="http://docbook.org/ns/docbook">
+  <!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+  <title>Apache HBase Configuration</title>
+  <para>This chapter expands upon the <xref linkend="getting_started" /> chapter to further explain
+    configuration of Apache HBase. Please read this chapter carefully, especially <xref
+      linkend="basic.prerequisites" /> to ensure that your HBase testing and deployment goes
+    smoothly, and prevent data loss.</para>
+
+  <para> Apache HBase uses the same configuration system as Apache Hadoop. All configuration files
+    are located in the <filename>conf/</filename> directory, which needs to be kept in sync for each
+    node on your cluster.</para>
+  
+  <variablelist>
+    <title>HBase Configuration Files</title>
+    <varlistentry>
+      <term><filename>backup-masters</filename></term>
+      <listitem>
+        <para>Not present by default. A plain-text file which lists hosts on which the Master should
+          start a backup Master process, one host per line.</para>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>hadoop-metrics2-hbase.properties</filename></term>
+      <listitem>
+        <para>Used to connect HBase Hadoop's Metrics2 framework. See the <link
+            xlink:href="http://wiki.apache.org/hadoop/HADOOP-6728-MetricsV2">Hadoop Wiki
+            entry</link> for more information on Metrics2. Contains only commented-out examples by
+          default.</para>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>hbase-env.cmd</filename> and <filename>hbase-env.sh</filename></term>
+      <listitem>
+        <para>Script for Windows and Linux / Unix environments to set up the working environment for
+        HBase, including the location of Java, Java options, and other environment variables. The
+        file contains many commented-out examples to provide guidance.</para>
+        <note>
+          <para>In HBase 0.98.5 and newer, you must set <envar>JAVA_HOME</envar> on each node of
+            your cluster. <filename>hbase-env.sh</filename> provides a handy mechanism to do
+            this.</para>
+        </note>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>hbase-policy.xml</filename></term>
+      <listitem>
+        <para>The default policy configuration file used by RPC servers to make authorization
+          decisions on client requests. Only used if HBase security (<xref
+            linkend="security" />) is enabled.</para>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>hbase-site.xml</filename></term>
+      <listitem>
+        <para>The main HBase configuration file. This file specifies configuration options which
+          override HBase's default configuration. You can view (but do not edit) the default
+          configuration file at <filename>docs/hbase-default.xml</filename>. You can also view the
+          entire effective configuration for your cluster (defaults and overrides) in the
+            <guilabel>HBase Configuration</guilabel> tab of the HBase Web UI.</para>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>log4j.properties</filename></term>
+      <listitem>
+        <para>Configuration file for HBase logging via <code>log4j</code>.</para>
+      </listitem>
+    </varlistentry>
+    <varlistentry>
+      <term><filename>regionservers</filename></term>
+      <listitem>
+        <para>A plain-text file containing a list of hosts which should run a RegionServer in your
+          HBase cluster. By default this file contains the single entry
+          <literal>localhost</literal>. It should contain a list of hostnames or IP addresses, one
+          per line, and should only contain <literal>localhost</literal> if each node in your
+          cluster will run a RegionServer on its <literal>localhost</literal> interface.</para>
+      </listitem>
+    </varlistentry>
+  </variablelist>
+  
+  <tip>
+    <title>Checking XML Validity</title>
+    <para>When you edit XML, it is a good idea to use an XML-aware editor to be sure that your
+      syntax is correct and your XML is well-formed. You can also use the <command>xmllint</command>
+      utility to check that your XML is well-formed. By default, <command>xmllint</command> re-flows
+      and prints the XML to standard output. To check for well-formedness and only print output if
+      errors exist, use the command <command>xmllint -noout
+        <replaceable>filename.xml</replaceable></command>.</para>
+  </tip>
+
+  <warning>
+    <title>Keep Configuration In Sync Across the Cluster</title>
+    <para>When running in distributed mode, after you make an edit to an HBase configuration, make
+      sure you copy the content of the <filename>conf/</filename> directory to all nodes of the
+      cluster. HBase will not do this for you. Use <command>rsync</command>, <command>scp</command>,
+      or another secure mechanism for copying the configuration files to your nodes. For most
+      configuration, a restart is needed for servers to pick up changes An exception is dynamic
+      configuration. to be described later below.</para>
+  </warning>
+
+  <section
+    xml:id="basic.prerequisites">
+    <title>Basic Prerequisites</title>
+    <para>This section lists required services and some required system configuration. </para>
+
+    <table
+      xml:id="java">
+      <title>Java</title>
+      <textobject>
+        <para>HBase requires at least Java 6 from <link
+            xlink:href="http://www.java.com/download/">Oracle</link>. The following table lists
+          which JDK version are compatible with each version of HBase.</para>
+      </textobject>
+      <tgroup
+        cols="4">
+        <thead>
+          <row>
+            <entry>HBase Version</entry>
+            <entry>JDK 6</entry>
+            <entry>JDK 7</entry>
+            <entry>JDK 8</entry>
+          </row>
+        </thead>
+        <tbody>
+          <row>
+            <entry>1.0</entry>
+            <entry><link
+                xlink:href="http://search-hadoop.com/m/DHED4Zlz0R1">Not Supported</link></entry>
+            <entry>yes</entry>
+            <entry><para>Running with JDK 8 will work but is not well tested.</para></entry>
+          </row>
+          <row>
+            <entry>0.98</entry>
+            <entry>yes</entry>
+            <entry>yes</entry>
+            <entry><para>Running with JDK 8 works but is not well tested. Building with JDK 8 would
+                require removal of the deprecated remove() method of the PoolMap class and is under
+                consideration. See ee <link
+                  xlink:href="https://issues.apache.org/jira/browse/HBASE-7608">HBASE-7608</link>
+                for more information about JDK 8 support.</para></entry>
+          </row>
+          <row>
+            <entry>0.96</entry>
+            <entry>yes</entry>
+            <entry>yes</entry>
+            <entry />
+          </row>
+          <row>
+            <entry>0.94</entry>
+            <entry>yes</entry>
+            <entry>yes</entry>
+            <entry />
+          </row>
+        </tbody>
+      </tgroup>
+    </table>
+
+    <note>
+      <para>In HBase 0.98.5 and newer, you must set <envar>JAVA_HOME</envar> on each node of
+        your cluster. <filename>hbase-env.sh</filename> provides a handy mechanism to do
+        this.</para>
+    </note>
+    
+    <variablelist
+      xml:id="os">
+      <title>Operating System Utilities</title>
+      <varlistentry
+        xml:id="ssh">
+        <term>ssh</term>
+        <listitem>
+          <para>HBase uses the Secure Shell (ssh) command and utilities extensively to communicate
+            between cluster nodes. Each server in the cluster must be running <command>ssh</command>
+            so that the Hadoop and HBase daemons can be managed. You must be able to connect to all
+            nodes via SSH, including the local node, from the Master as well as any backup Master,
+            using a shared key rather than a password. You can see the basic methodology for such a
+            set-up in Linux or Unix systems at <xref
+              linkend="passwordless.ssh.quickstart" />. If your cluster nodes use OS X, see the
+            section, <link
+              xlink:href="http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29">SSH:
+              Setting up Remote Desktop and Enabling Self-Login</link> on the Hadoop wiki.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry
+        xml:id="dns">
+        <term>DNS</term>
+        <listitem>
+          <para>HBase uses the local hostname to self-report its IP address. Both forward and
+            reverse DNS resolving must work in versions of HBase previous to 0.92.0. The <link
+                  xlink:href="https://github.com/sujee/hadoop-dns-checker">hadoop-dns-checker</link>
+                tool can be used to verify DNS is working correctly on the cluster. The project
+                README file provides detailed instructions on usage. </para>
+
+          <para>If your server has multiple network interfaces, HBase defaults to using the
+            interface that the primary hostname resolves to. To override this behavior, set the
+              <code>hbase.regionserver.dns.interface</code> property to a different interface. This
+            will only work if each server in your cluster uses the same network interface
+            configuration.</para>
+
+          <para>To choose a different DNS nameserver than the system default, set the
+              <varname>hbase.regionserver.dns.nameserver</varname> property to the IP address of
+            that nameserver.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry
+        xml:id="loopback.ip">
+        <term>Loopback IP</term>
+        <listitem>
+          <para>Prior to hbase-0.96.0, HBase only used the IP address
+              <systemitem>127.0.0.1</systemitem> to refer to <code>localhost</code>, and this could
+            not be configured. See <xref
+              linkend="loopback.ip" />.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry
+        xml:id="ntp">
+        <term>NTP</term>
+        <listitem>
+          <para>The clocks on cluster nodes should be synchronized. A small amount of variation is
+            acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time
+            synchronization is one of the first things to check if you see unexplained problems in
+            your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or
+            another time-synchronization mechanism, on your cluster, and that all nodes look to the
+            same service for time synchronization. See the <link
+              xlink:href="http://www.tldp.org/LDP/sag/html/basic-ntp-config.html">Basic NTP
+              Configuration</link> at <citetitle>The Linux Documentation Project (TLDP)</citetitle>
+            to set up NTP.</para>
+        </listitem>
+      </varlistentry>
+
+      <varlistentry
+        xml:id="ulimit">
+        <term>Limits on Number of Files and Processes (<command>ulimit</command>)
+          <indexterm>
+            <primary>ulimit</primary>
+          </indexterm><indexterm>
+            <primary>nproc</primary>
+          </indexterm>
+        </term>
+
+        <listitem>
+          <para>Apache HBase is a database. It requires the ability to open a large number of files
+            at once. Many Linux distributions limit the number of files a single user is allowed to
+            open to <literal>1024</literal> (or <literal>256</literal> on older versions of OS X).
+            You can check this limit on your servers by running the command <command>ulimit
+              -n</command> when logged in as the user which runs HBase. See <xref
+              linkend="trouble.rs.runtime.filehandles" /> for some of the problems you may
+            experience if the limit is too low. You may also notice errors such as the
+            following:</para>
+          <screen>
+2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
+2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
+          </screen>
+          <para>It is recommended to raise the ulimit to at least 10,000, but more likely 10,240,
+            because the value is usually expressed in multiples of 1024. Each ColumnFamily has at
+            least one StoreFile, and possibly more than 6 StoreFiles if the region is under load.
+            The number of open files required depends upon the number of ColumnFamilies and the
+            number of regions. The following is a rough formula for calculating the potential number
+            of open files on a RegionServer. </para>
+          <example>
+            <title>Calculate the Potential Number of Open Files</title>
+            <screen>(StoreFiles per ColumnFamily) x (regions per RegionServer)</screen>
+          </example>
+          <para>For example, assuming that a schema had 3 ColumnFamilies per region with an average
+            of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM
+            will open 3 * 3 * 100 = 900 file descriptors, not counting open JAR files, configuration
+            files, and others. Opening a file does not take many resources, and the risk of allowing
+            a user to open too many files is minimal.</para>
+          <para>Another related setting is the number of processes a user is allowed to run at once.
+            In Linux and Unix, the number of processes is set using the <command>ulimit -u</command>
+            command. This should not be confused with the <command>nproc</command> command, which
+            controls the number of CPUs available to a given user. Under load, a
+              <varname>nproc</varname> that is too low can cause OutOfMemoryError exceptions. See
+            Jack Levin's <link
+              xlink:href="http://thread.gmane.org/gmane.comp.java.hadoop.hbase.user/16374">major
+              hdfs issues</link> thread on the hbase-users mailing list, from 2011.</para>
+          <para>Configuring the fmaximum number of ile descriptors and processes for the user who is
+            running the HBase process is an operating system configuration, rather than an HBase
+            configuration. It is also important to be sure that the settings are changed for the
+            user that actually runs HBase. To see which user started HBase, and that user's ulimit
+            configuration, look at the first line of the HBase log for that instance. A useful read
+            setting config on you hadoop cluster is Aaron Kimballs' <link
+              xlink:href="http://www.cloudera.com/blog/2009/03/configuration-parameters-what-can-you-just-ignore/"
+              >Configuration Parameters: What can you just ignore?</link></para>
+          <formalpara xml:id="ulimit_ubuntu">
+            <title><command>ulimit</command> Settings on Ubuntu</title>
+            <para>To configure <command>ulimit</command> settings on Ubuntu, edit
+                <filename>/etc/security/limits.conf</filename>, which is a space-delimited file with
+              four columns. Refer to the <link
+                xlink:href="http://manpages.ubuntu.com/manpages/lucid/man5/limits.conf.5.html">man
+                page for limits.conf</link> for details about the format of this file. In the
+              following example, the first line sets both soft and hard limits for the number of
+              open files (<literal>nofile</literal>) to <literal>32768</literal> for the operating
+              system user with the username <literal>hadoop</literal>. The second line sets the
+              number of processes to 32000 for the same user.</para>
+          </formalpara>
+          <screen>
+hadoop  -       nofile  32768
+hadoop  -       nproc   32000
+          </screen>
+          <para>The settings are only applied if the Pluggable Authentication Module (PAM)
+            environment is directed to use them. To configure PAM to use these limits, be sure that
+            the <filename>/etc/pam.d/common-session</filename> file contains the following line:</para>
+          <screen>session required  pam_limits.so</screen>
+        </listitem>
+      </varlistentry>
+
+      <varlistentry
+        xml:id="windows">
+        <term>Windows</term>
+
+        <listitem>
+          <para>Prior to HBase 0.96, testing for running HBase on Microsoft Windows was limited.
+            Running a on Windows nodes is not recommended for production systems.</para>
+
+        <para>To run versions of HBase prior to 0.96 on Microsoft Windows, you must install <link
+            xlink:href="http://cygwin.com/">Cygwin</link> and run HBase within the Cygwin
+          environment. This provides support for Linux/Unix commands and scripts. The full details are explained in the <link
+            xlink:href="http://hbase.apache.org/cygwin.html">Windows Installation</link> guide. Also <link
+            xlink:href="http://search-hadoop.com/?q=hbase+windows&amp;fc_project=HBase&amp;fc_type=mail+_hash_+dev">search
+            our user mailing list</link> to pick up latest fixes figured by Windows users.</para>
+        <para>Post-hbase-0.96.0, hbase runs natively on windows with supporting
+            <command>*.cmd</command> scripts bundled. </para></listitem>
+      </varlistentry>
+
+    </variablelist>
+    <!--  OS -->
+
+    <section
+      xml:id="hadoop">
+      <title><link
+          xlink:href="http://hadoop.apache.org">Hadoop</link><indexterm>
+          <primary>Hadoop</primary>
+        </indexterm></title>
+      <para>The following table summarizes the versions of Hadoop supported with each version of
+        HBase. Based on the version of HBase, you should select the most
+        appropriate version of Hadoop. You can use Apache Hadoop, or a vendor's distribution of
+        Hadoop. No distinction is made here. See <link
+          xlink:href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" />
+        for information about vendors of Hadoop.</para>
+      <tip>
+        <title>Hadoop 2.x is recommended.</title>
+        <para>Hadoop 2.x is faster and includes features, such as short-circuit reads, which will
+          help improve your HBase random read profile. Hadoop 2.x also includes important bug fixes
+          that will improve your overall HBase experience. HBase 0.98 drops support for Hadoop 1.0, deprecates use of Hadoop 1.1+,
+          and HBase 1.0 will not support Hadoop 1.x.</para>
+      </tip>
+      <para>Use the following legend to interpret this table:</para>
+      <simplelist
+        type="vert"
+        columns="1">
+        <member>S = supported and tested,</member>
+        <member>X = not supported,</member>
+        <member>NT = it should run, but not tested enough.</member>
+      </simplelist>
+
+      <table>
+        <title>Hadoop version support matrix</title>
+        <tgroup
+          cols="6"
+          align="left"
+          colsep="1"
+          rowsep="1">
+          <colspec
+            colname="c1"
+            align="left" />
+          <colspec
+            colname="c2"
+            align="center" />
+          <colspec
+            colname="c3"
+            align="center" />
+          <colspec
+            colname="c4"
+            align="center" />
+          <colspec
+            colname="c5"
+            align="center" />
+          <colspec
+            colname="c6"
+            align="center" />
+          <thead>
+            <row>
+              <entry> </entry>
+              <entry>HBase-0.92.x</entry>
+              <entry>HBase-0.94.x</entry>
+              <entry>HBase-0.96.x</entry>
+              <entry><para>HBase-0.98.x (Support for Hadoop 1.1+ is deprecated.)</para></entry>
+              <entry><para>HBase-1.0.x (Hadoop 1.x is NOT supported)</para></entry>
+            </row>
+          </thead>
+          <tbody>
+            <row>
+              <entry>Hadoop-0.20.205</entry>
+              <entry>S</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-0.22.x </entry>
+              <entry>S</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-1.0.x</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-1.1.x </entry>
+              <entry>NT</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+              <entry>NT</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-0.23.x </entry>
+              <entry>X</entry>
+              <entry>S</entry>
+              <entry>NT</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.0.x-alpha </entry>
+              <entry>X</entry>
+              <entry>NT</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.1.0-beta </entry>
+              <entry>X</entry>
+              <entry>NT</entry>
+              <entry>S</entry>
+              <entry>X</entry>
+              <entry>X</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.2.0 </entry>
+              <entry>X</entry>
+              <entry><link linkend="hadoop2.hbase-0.94">NT</link></entry>
+              <entry>S</entry>
+              <entry>S</entry>
+              <entry>NT</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.3.x</entry>
+              <entry>X</entry>
+              <entry>NT</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+              <entry>NT</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.4.x</entry>
+              <entry>X</entry>
+              <entry>NT</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+            </row>
+            <row>
+              <entry>Hadoop-2.5.x</entry>
+              <entry>X</entry>
+              <entry>NT</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+              <entry>S</entry>
+            </row>
+
+          </tbody>
+        </tgroup>
+      </table>
+
+      <note
+        xml:id="replace.hadoop">
+        <title>Replace the Hadoop Bundled With HBase!</title>
+        <para> Because HBase depends on Hadoop, it bundles an instance of the Hadoop jar under its
+            <filename>lib</filename> directory. The bundled jar is ONLY for use in standalone mode.
+          In distributed mode, it is <emphasis>critical</emphasis> that the version of Hadoop that
+          is out on your cluster match what is under HBase. Replace the hadoop jar found in the
+          HBase lib directory with the hadoop jar you are running on your cluster to avoid version
+          mismatch issues. Make sure you replace the jar in HBase everywhere on your cluster. Hadoop
+          version mismatch issues have various manifestations but often all looks like its hung up.
+        </para>
+      </note>
+      <section
+        xml:id="hadoop2.hbase-0.94">
+        <title>Apache HBase 0.94 with Hadoop 2</title>
+        <para>To get 0.94.x to run on hadoop 2.2.0, you need to change the hadoop
+        2 and protobuf versions in the <filename>pom.xml</filename>: Here is a diff with
+        pom.xml changes: </para>
+        <programlisting><![CDATA[$ svn diff pom.xml
+Index: pom.xml
+===================================================================
+--- pom.xml     (revision 1545157)
++++ pom.xml     (working copy)
+@@ -1034,7 +1034,7 @@
+     <slf4j.version>1.4.3</slf4j.version>
+     <log4j.version>1.2.16</log4j.version>
+     <mockito-all.version>1.8.5</mockito-all.version>
+-    <protobuf.version>2.4.0a</protobuf.version>
++    <protobuf.version>2.5.0</protobuf.version>
+     <stax-api.version>1.0.1</stax-api.version>
+     <thrift.version>0.8.0</thrift.version>
+     <zookeeper.version>3.4.5</zookeeper.version>
+@@ -2241,7 +2241,7 @@
+         </property>
+       </activation>
+       <properties>
+-        <hadoop.version>2.0.0-alpha</hadoop.version>
++        <hadoop.version>2.2.0</hadoop.version>
+         <slf4j.version>1.6.1</slf4j.version>
+       </properties>
+       <dependencies>]]>
+                   </programlisting>
+                  <para>The next step is to regenerate Protobuf files and assuming that the Protobuf
+                    has been installed:</para>
+                  <itemizedlist>
+                    <listitem>
+                      <para>Go to the hbase root folder, using the command line;</para>
+                    </listitem>
+                    <listitem>
+                      <para>Type the following commands:</para>
+                      <para>
+                        <programlisting language="bourne"><![CDATA[$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/hbase.proto]]></programlisting>
+                      </para>
+                      <para>
+                        <programlisting language="bourne"><![CDATA[$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/ErrorHandling.proto]]></programlisting>
+                      </para>
+                    </listitem>
+                  </itemizedlist>
+                  <para> Building against the hadoop 2 profile by running something like the
+                    following command: </para>
+                  <screen language="bourne">$  mvn clean install assembly:single -Dhadoop.profile=2.0 -DskipTests</screen>
+      </section>
+      <section
+        xml:id="hadoop.hbase-0.94">
+        <title>Apache HBase 0.92 and 0.94</title>
+        <para>HBase 0.92 and 0.94 versions can work with Hadoop versions, 0.20.205, 0.22.x, 1.0.x,
+          and 1.1.x. HBase-0.94 can additionally work with Hadoop-0.23.x and 2.x, but you may have
+          to recompile the code using the specific maven profile (see top level pom.xml)</para>
+      </section>
+
+      <section
+        xml:id="hadoop.hbase-0.96">
+        <title>Apache HBase 0.96</title>
+        <para> As of Apache HBase 0.96.x, Apache Hadoop 1.0.x at least is required. Hadoop 2 is
+          strongly encouraged (faster but also has fixes that help MTTR). We will no longer run
+          properly on older Hadoops such as 0.20.205 or branch-0.20-append. Do not move to Apache
+          HBase 0.96.x if you cannot upgrade your Hadoop.. See <link
+                xlink:href="http://search-hadoop.com/m/7vFVx4EsUb2">HBase, mail # dev - DISCUSS:
+                Have hbase require at least hadoop 1.0.0 in hbase 0.96.0?</link></para>
+      </section>
+
+      <section
+        xml:id="hadoop.older.versions">
+        <title>Hadoop versions 0.20.x - 1.x</title>
+        <para> HBase will lose data unless it is running on an HDFS that has a durable
+            <code>sync</code> implementation. DO NOT use Hadoop 0.20.2, Hadoop 0.20.203.0, and
+          Hadoop 0.20.204.0 which DO NOT have this attribute. Currently only Hadoop versions
+          0.20.205.x or any release in excess of this version -- this includes hadoop-1.0.0 -- have
+          a working, durable sync. The Cloudera blog post <link
+            xlink:href="http://www.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/">An
+            update on Apache Hadoop 1.0</link> by Charles Zedlweski has a nice exposition on how all
+          the Hadoop versions relate. Its worth checking out if you are having trouble making sense
+          of the Hadoop version morass. </para>
+        <para>Sync has to be explicitly enabled by setting
+            <varname>dfs.support.append</varname> equal to true on both the client side -- in
+            <filename>hbase-site.xml</filename> -- and on the serverside in
+            <filename>hdfs-site.xml</filename> (The sync facility HBase needs is a subset of the
+          append code path).</para>
+        <programlisting language="xml"><![CDATA[  
+<property>
+  <name>dfs.support.append</name>
+  <value>true</value>
+</property>]]></programlisting>
+        <para> You will have to restart your cluster after making this edit. Ignore the
+          chicken-little comment you'll find in the <filename>hdfs-default.xml</filename> in the
+          description for the <varname>dfs.support.append</varname> configuration. </para>
+      </section>
+      <section
+        xml:id="hadoop.security">
+        <title>Apache HBase on Secure Hadoop</title>
+        <para>Apache HBase will run on any Hadoop 0.20.x that incorporates Hadoop security features
+          as long as you do as suggested above and replace the Hadoop jar that ships with HBase with
+          the secure version. If you want to read more about how to setup Secure HBase, see <xref
+            linkend="hbase.secure.configuration" />.</para>
+      </section>
+
+      <section
+        xml:id="dfs.datanode.max.transfer.threads">
+        <title><varname>dfs.datanode.max.transfer.threads</varname><indexterm>
+            <primary>dfs.datanode.max.transfer.threads</primary>
+          </indexterm></title>
+
+        <para>An HDFS datanode has an upper bound on the number of files that it will serve
+          at any one time. Before doing any loading, make sure you have configured
+          Hadoop's <filename>conf/hdfs-site.xml</filename>, setting the
+          <varname>dfs.datanode.max.transfer.threads</varname> value to at least the following:
+        </para>
+        <programlisting language="xml"><![CDATA[
+<property>
+  <name>dfs.datanode.max.transfer.threads</name>
+  <value>4096</value>
+</property>
+      ]]></programlisting>
+
+        <para>Be sure to restart your HDFS after making the above configuration.</para>
+
+        <para>Not having this configuration in place makes for strange-looking failures. One
+        manifestation is a complaint about missing blocks. For example:</para>
+        <screen>10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
+          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes
+          contain current block. Will get new block locations from namenode and retry...</screen>
+        <para>See also <xref linkend="casestudies.max.transfer.threads" /> and note that this
+          property was previously known as <varname>dfs.datanode.max.xcievers</varname> (e.g.
+          <link
+            xlink:href="http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html">
+            Hadoop HDFS: Deceived by Xciever</link>).
+        </para>
+
+
+      </section>
+    </section>
+    <!--  hadoop -->
+    <section xml:id="zookeeper.requirements">
+      <title>ZooKeeper Requirements</title>
+      <para>ZooKeeper 3.4.x is required as of HBase 1.0.0. HBase makes use of the
+        <methodname>multi</methodname> functionality that is only available since 3.4.0
+        (The <property>useMulti</property> is defaulted true in HBase 1.0.0).
+        See <link href="https://issues.apache.org/jira/browse/HBASE-12241">HBASE-12241 The crash of regionServer when taking deadserver's replication queue breaks replication</link>
+        and <link href="https://issues.apache.org/jira/browse/HBASE-6775">Use ZK.multi when available for HBASE-6710 0.92/0.94 compatibility fix</link> for background.</para>
+    </section>
+  </section>
+
+  <section
+    xml:id="standalone_dist">
+    <title>HBase run modes: Standalone and Distributed</title>
+
+    <para>HBase has two run modes: <xref
+        linkend="standalone" /> and <xref
+        linkend="distributed" />. Out of the box, HBase runs in standalone mode. Whatever your mode,
+      you will need to configure HBase by editing files in the HBase <filename>conf</filename>
+      directory. At a minimum, you must edit <code>conf/hbase-env.sh</code> to tell HBase which
+        <command>java</command> to use. In this file you set HBase environment variables such as the
+      heapsize and other options for the <application>JVM</application>, the preferred location for
+      log files, etc. Set <varname>JAVA_HOME</varname> to point at the root of your
+        <command>java</command> install.</para>
+
+    <section
+      xml:id="standalone">
+      <title>Standalone HBase</title>
+
+      <para>This is the default mode. Standalone mode is what is described in the <xref
+          linkend="quickstart" /> section. In standalone mode, HBase does not use HDFS -- it uses
+        the local filesystem instead -- and it runs all HBase daemons and a local ZooKeeper all up
+        in the same JVM. Zookeeper binds to a well known port so clients may talk to HBase.</para>
+    </section>
+
+    <section
+      xml:id="distributed">
+      <title>Distributed</title>
+
+      <para>Distributed mode can be subdivided into distributed but all daemons run on a single node
+        -- a.k.a <emphasis>pseudo-distributed</emphasis>-- and
+          <emphasis>fully-distributed</emphasis> where the daemons are spread across all nodes in
+        the cluster. The pseudo-distributed vs fully-distributed nomenclature comes from Hadoop.</para>
+
+      <para>Pseudo-distributed mode can run against the local filesystem or it can run against an
+        instance of the <emphasis>Hadoop Distributed File System</emphasis> (HDFS).
+        Fully-distributed mode can ONLY run on HDFS. See the Hadoop <link
+          xlink:href="http://hadoop.apache.org/common/docs/r1.1.1/api/overview-summary.html#overview_description">
+          requirements and instructions</link> for how to set up HDFS for Hadoop 1.x. A good
+        walk-through for setting up HDFS on Hadoop 2 is at <link
+          xlink:href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide">http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide</link>.</para>
+
+      <para>Below we describe the different distributed setups. Starting, verification and
+        exploration of your install, whether a <emphasis>pseudo-distributed</emphasis> or
+          <emphasis>fully-distributed</emphasis> configuration is described in a section that
+        follows, <xref
+          linkend="confirm" />. The same verification script applies to both deploy types.</para>
+      <section
+        xml:id="pseudo">
+        <title>Pseudo-distributed</title>
+        <note>
+          <title>Pseudo-Distributed Quickstart</title>
+          <para>A quickstart has been added to the <xref
+              linkend="quickstart" /> chapter. See <xref
+              linkend="quickstart-pseudo" />. Some of the information that was originally in this
+            section has been moved there.</para>
+        </note>
+
+        <para>A pseudo-distributed mode is simply a fully-distributed mode run on a single host. Use
+          this configuration testing and prototyping on HBase. Do not use this configuration for
+          production nor for evaluating HBase performance.</para>
+
+      </section>
+
+    </section>
+
+    <section
+      xml:id="fully_dist">
+      <title>Fully-distributed</title>
+      <para>By default, HBase runs in standalone mode. Both standalone mode and pseudo-distributed
+        mode are provided for the purposes of small-scale testing. For a production environment,
+        distributed mode is appropriate. In distributed mode, multiple instances of HBase daemons
+        run on multiple servers in the cluster.</para>
+      <para>Just as in pseudo-distributed mode, a fully distributed configuration requires that you
+        set the <code>hbase-cluster.distributed</code> property to <literal>true</literal>.
+        Typically, the <code>hbase.rootdir</code> is configured to point to a highly-available HDFS
+        filesystem. </para>
+      <para>In addition, the cluster is configured so that multiple cluster nodes enlist as
+        RegionServers, ZooKeeper QuorumPeers, and backup HMaster servers. These configuration basics
+        are all demonstrated in <xref
+          linkend="quickstart-fully-distributed" />.</para>
+
+      <formalpara
+        xml:id="regionserver">
+        <title>Distributed RegionServers</title>
+        <para>Typically, your cluster will contain multiple RegionServers all running on different
+          servers, as well as primary and backup Master and Zookeeper daemons. The
+            <filename>conf/regionservers</filename> file on the master server contains a list of
+          hosts whose RegionServers are associated with this cluster. Each host is on a separate
+          line. All hosts listed in this file will have their RegionServer processes started and
+          stopped when the master server starts or stops.</para>
+      </formalpara>
+
+      <formalpara
+        xml:id="hbase.zookeeper">
+        <title>ZooKeeper and HBase</title>
+        <para>See section <xref
+            linkend="zookeeper" /> for ZooKeeper setup for HBase.</para>
+      </formalpara>
+
+      <example>
+        <title>Example Distributed HBase Cluster</title>
+        <para>This is a bare-bones <filename>conf/hbase-site.xml</filename> for a distributed HBase
+          cluster. A cluster that is used for real-world work would contain more custom
+          configuration parameters. Most HBase configuration directives have default values, which
+          are used unless the value is overridden in the <filename>hbase-site.xml</filename>. See <xref
+            linkend="config.files" /> for more information.</para>
+        <programlisting language="xml"><![CDATA[
+<configuration>
+  <property>
+    <name>hbase.rootdir</name>
+    <value>hdfs://namenode.example.org:8020/hbase</value>
+  </property>
+  <property>
+    <name>hbase.cluster.distributed</name>
+    <value>true</value>
+  </property>
+  <property>
+      <name>hbase.zookeeper.quorum</name>
+      <value>node-a.example.com,node-b.example.com,node-c.example.com</value>
+    </property>
+</configuration>
+]]>
+        </programlisting>
+        <para>This is an example <filename>conf/regionservers</filename> file, which contains a list
+          of each node that should run a RegionServer in the cluster. These nodes need HBase
+          installed and they need to use the same contents of the <filename>conf/</filename>
+          directory as the Master server..</para>
+        <programlisting>
+node-a.example.com
+node-b.example.com
+node-c.example.com
+        </programlisting>
+        <para>This is an example <filename>conf/backup-masters</filename> file, which contains a
+          list of each node that should run a backup Master instance. The backup Master instances
+          will sit idle unless the main Master becomes unavailable.</para>
+        <programlisting>
+node-b.example.com
+node-c.example.com
+        </programlisting>
+      </example>
+      <formalpara>
+        <title>Distributed HBase Quickstart</title>
+        <para>See <xref
+            linkend="quickstart-fully-distributed" /> for a walk-through of a simple three-node
+          cluster configuration with multiple ZooKeeper, backup HMaster, and RegionServer
+          instances.</para>
+      </formalpara>
+
+      <procedure
+        xml:id="hdfs_client_conf">
+        <title>HDFS Client Configuration</title>
+        <step>
+          <para>Of note, if you have made HDFS client configuration on your Hadoop cluster, such as
+            configuration directives for HDFS clients, as opposed to server-side configurations, you
+            must use one of the following methods to enable HBase to see and use these configuration
+            changes:</para>
+          <stepalternatives>
+            <step>
+              <para>Add a pointer to your <varname>HADOOP_CONF_DIR</varname> to the
+                  <varname>HBASE_CLASSPATH</varname> environment variable in
+                  <filename>hbase-env.sh</filename>.</para>
+            </step>
+
+            <step>
+              <para>Add a copy of <filename>hdfs-site.xml</filename> (or
+                  <filename>hadoop-site.xml</filename>) or, better, symlinks, under
+                  <filename>${HBASE_HOME}/conf</filename>, or</para>
+            </step>
+
+            <step>
+              <para>if only a small set of HDFS client configurations, add them to
+                  <filename>hbase-site.xml</filename>.</para>
+            </step>
+          </stepalternatives>
+        </step>
+      </procedure>
+      <para>An example of such an HDFS client configuration is <varname>dfs.replication</varname>.
+        If for example, you want to run with a replication factor of 5, hbase will create files with
+        the default of 3 unless you do the above to make the configuration available to
+        HBase.</para>
+    </section>
+  </section>
+
+    <section
+      xml:id="confirm">
+      <title>Running and Confirming Your Installation</title>
+
+
+
+      <para>Make sure HDFS is running first. Start and stop the Hadoop HDFS daemons by running
+          <filename>bin/start-hdfs.sh</filename> over in the <varname>HADOOP_HOME</varname>
+        directory. You can ensure it started properly by testing the <command>put</command> and
+          <command>get</command> of files into the Hadoop filesystem. HBase does not normally use
+        the mapreduce daemons. These do not need to be started.</para>
+      <para><emphasis>If</emphasis> you are managing your own ZooKeeper, start it and confirm its
+        running else, HBase will start up ZooKeeper for you as part of its start process.</para>
+      <para>Start HBase with the following command:</para>
+      <screen>bin/start-hbase.sh</screen>
+      <para>Run the above from the <varname>HBASE_HOME</varname> directory.</para>
+      <para>You should now have a running HBase instance. HBase logs can be found in the
+          <filename>logs</filename> subdirectory. Check them out especially if HBase had trouble
+        starting.</para>
+
+      <para>HBase also puts up a UI listing vital attributes. By default its deployed on the Master
+        host at port 60010 (HBase RegionServers listen on port 60020 by default and put up an
+        informational http server at 60030). If the Master were running on a host named
+          <varname>master.example.org</varname> on the default port, to see the Master's homepage
+        you'd point your browser at <filename>http://master.example.org:60010</filename>.</para>
+
+      <para>Prior to HBase 0.98, the default ports the master ui was deployed on port 60010, and the
+        HBase RegionServers would listen on port 60020 by default and put up an informational http
+        server at 60030. </para>
+
+      <para>Once HBase has started, see the <xref
+          linkend="shell_exercises" /> for how to create tables, add data, scan your insertions, and
+        finally disable and drop your tables.</para>
+
+      <para>To stop HBase after exiting the HBase shell enter</para>
+      <screen language="bourne">$ ./bin/stop-hbase.sh
+stopping hbase...............</screen>
+      <para>Shutdown can take a moment to complete. It can take longer if your cluster is comprised
+        of many machines. If you are running a distributed operation, be sure to wait until HBase
+        has shut down completely before stopping the Hadoop daemons.</para>
+    </section>
+
+  <!--  run modes -->
+
+
+
+  <section
+    xml:id="config.files">
+    <title>Configuration Files</title>
+
+    <section
+      xml:id="hbase.site">
+      <title><filename>hbase-site.xml</filename> and <filename>hbase-default.xml</filename></title>
+      <para>Just as in Hadoop where you add site-specific HDFS configuration to the
+          <filename>hdfs-site.xml</filename> file, for HBase, site specific customizations go into
+        the file <filename>conf/hbase-site.xml</filename>. For the list of configurable properties,
+        see <xref
+          linkend="hbase_default_configurations" /> below or view the raw
+          <filename>hbase-default.xml</filename> source file in the HBase source code at
+          <filename>src/main/resources</filename>. </para>
+      <para> Not all configuration options make it out to <filename>hbase-default.xml</filename>.
+        Configuration that it is thought rare anyone would change can exist only in code; the only
+        way to turn up such configurations is via a reading of the source code itself. </para>
+      <para> Currently, changes here will require a cluster restart for HBase to notice the change. </para>
+      <!--The file hbase-default.xml is generated as part of
+    the build of the hbase site.  See the hbase pom.xml.
+    The generated file is a docbook section with a glossary
+    in it-->
+      <!--presumes the pre-site target has put the hbase-default.xml at this location-->
+      <xi:include
+        xmlns:xi="http://www.w3.org/2001/XInclude"
+        href="../../../target/docbkx/hbase-default.xml">
+        <xi:fallback>
+          <section
+            xml:id="hbase_default_configurations">
+            <title />
+            <para>
+              <emphasis>This file is fallback content</emphasis>. If you are seeing this, something
+              is wrong with the build of the HBase documentation or you are doing pre-build
+              verification. </para>
+            <para> The file hbase-default.xml is generated as part of the build of the hbase site.
+              See the hbase <filename>pom.xml</filename>. The generated file is a docbook glossary. </para>
+            <section>
+              <title>IDs that are auto-generated and cause validation errors if not present</title>
+              <para> Each of these is a reference to a configuration file parameter which will cause
+                an error if you are using the fallback content here. This is a dirty dirty hack. </para>
+              <section
+                xml:id="fail.fast.expired.active.master">
+                <title>fail.fast.expired.active.master</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hregion.memstore.flush.size">
+                <title>"hbase.hregion.memstore.flush.size"</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hstore.bytes.per.checksum">
+                <title>hbase.hstore.bytes.per.checksum</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.online.schema.update.enable">
+                <title>hbase.online.schema.update.enable</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.regionserver.global.memstore.size">
+                <title>hbase.regionserver.global.memstore.size</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hregion.max.filesize">
+                <title>hbase.hregion.max.filesize</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hstore.blockingStoreFiles">
+                <title>hbase.hstore.BlockingStoreFiles</title>
+                <para />
+              </section>
+              <section
+                xml:id="hfile.block.cache.size">
+                <title>hfile.block.cache.size</title>
+                <para />
+              </section>
+              <section
+                xml:id="copy.table">
+                <title>copy.table</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hstore.checksum.algorithm">
+                <title>hbase.hstore.checksum.algorithm</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.zookeeper.useMulti">
+                <title>hbase.zookeeper.useMulti</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.hregion.memstore.block.multiplier">
+                <title>hbase.hregion.memstore.block.multiplier</title>
+                <para />
+              </section>
+              <section
+                xml:id="hbase.regionserver.global.memstore.size.lower.limit">
+                <title>hbase.regionserver.global.memstore.size.lower.limit</title>
+                <para />
+              </section>
+            </section>
+          </section>
+        </xi:fallback>
+      </xi:include>
+    </section>
+
+    <section
+      xml:id="hbase.env.sh">
+      <title><filename>hbase-env.sh</filename></title>
+      <para>Set HBase environment variables in this file. Examples include options to pass the JVM
+        on start of an HBase daemon such as heap size and garbage collector configs. You can also
+        set configurations for HBase configuration, log directories, niceness, ssh options, where to
+        locate process pid files, etc. Open the file at <filename>conf/hbase-env.sh</filename> and
+        peruse its content. Each option is fairly well documented. Add your own environment
+        variables here if you want them read by HBase daemons on startup.</para>
+      <para> Changes here will require a cluster restart for HBase to notice the change. </para>
+    </section>
+
+    <section
+      xml:id="log4j">
+      <title><filename>log4j.properties</filename></title>
+      <para>Edit this file to change rate at which HBase files are rolled and to change the level at
+        which HBase logs messages. </para>
+      <para> Changes here will require a cluster restart for HBase to notice the change though log
+        levels can be changed for particular daemons via the HBase UI. </para>
+    </section>
+
+    <section
+      xml:id="client_dependencies">
+      <title>Client configuration and dependencies connecting to an HBase cluster</title>
+      <para>If you are running HBase in standalone mode, you don't need to configure anything for
+        your client to work provided that they are all on the same machine.</para>
+      <para> Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for
+        current critical locations. ZooKeeper is where all these values are kept. Thus clients
+        require the location of the ZooKeeper ensemble information before they can do anything else.
+        Usually this the ensemble location is kept out in the <filename>hbase-site.xml</filename>
+        and is picked up by the client from the <varname>CLASSPATH</varname>.</para>
+
+      <para>If you are configuring an IDE to run a HBase client, you should include the
+          <filename>conf/</filename> directory on your classpath so
+          <filename>hbase-site.xml</filename> settings can be found (or add
+          <filename>src/test/resources</filename> to pick up the hbase-site.xml used by tests). </para>
+      <para> Minimally, a client of HBase needs several libraries in its
+          <varname>CLASSPATH</varname> when connecting to a cluster, including:
+        <programlisting>
+commons-configuration (commons-configuration-1.6.jar)
+commons-lang (commons-lang-2.5.jar)
+commons-logging (commons-logging-1.1.1.jar)
+hadoop-core (hadoop-core-1.0.0.jar)
+hbase (hbase-0.92.0.jar)
+log4j (log4j-1.2.16.jar)
+slf4j-api (slf4j-api-1.5.8.jar)
+slf4j-log4j (slf4j-log4j12-1.5.8.jar)
+zookeeper (zookeeper-3.4.2.jar)</programlisting>
+      </para>
+      <para> An example basic <filename>hbase-site.xml</filename> for client only might look as
+        follows: <programlisting language="xml"><![CDATA[
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<configuration>
+  <property>
+    <name>hbase.zookeeper.quorum</name>
+    <value>example1,example2,example3</value>
+    <description>The directory shared by region servers.
+    </description>
+  </property>
+</configuration>
+]]></programlisting>
+      </para>
+
+      <section
+        xml:id="java.client.config">
+        <title>Java client configuration</title>
+        <para>The configuration used by a Java client is kept in an <link
+            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration">HBaseConfiguration</link>
+          instance. The factory method on HBaseConfiguration,
+            <code>HBaseConfiguration.create();</code>, on invocation, will read in the content of
+          the first <filename>hbase-site.xml</filename> found on the client's
+            <varname>CLASSPATH</varname>, if one is present (Invocation will also factor in any
+            <filename>hbase-default.xml</filename> found; an hbase-default.xml ships inside the
+            <filename>hbase.X.X.X.jar</filename>). It is also possible to specify configuration
+          directly without having to read from a <filename>hbase-site.xml</filename>. For example,
+          to set the ZooKeeper ensemble for the cluster programmatically do as follows:
+          <programlisting language="java">Configuration config = HBaseConfiguration.create();
+config.set("hbase.zookeeper.quorum", "localhost");  // Here we are running zookeeper locally</programlisting>
+          If multiple ZooKeeper instances make up your ZooKeeper ensemble, they may be specified in
+          a comma-separated list (just as in the <filename>hbase-site.xml</filename> file). This
+          populated <classname>Configuration</classname> instance can then be passed to an <link
+            xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html">HTable</link>,
+          and so on. </para>
+      </section>
+    </section>
+
+  </section>
+  <!--  config files -->
+
+  <section
+    xml:id="example_config">
+    <title>Example Configurations</title>
+
+    <section>
+      <title>Basic Distributed HBase Install</title>
+
+      <para>Here is an example basic configuration for a distributed ten node cluster. The nodes are
+        named <varname>example0</varname>, <varname>example1</varname>, etc., through node
+          <varname>example9</varname> in this example. The HBase Master and the HDFS namenode are
+        running on the node <varname>example0</varname>. RegionServers run on nodes
+          <varname>example1</varname>-<varname>example9</varname>. A 3-node ZooKeeper ensemble runs
+        on <varname>example1</varname>, <varname>example2</varname>, and <varname>example3</varname>
+        on the default ports. ZooKeeper data is persisted to the directory
+          <filename>/export/zookeeper</filename>. Below we show what the main configuration files --
+          <filename>hbase-site.xml</filename>, <filename>regionservers</filename>, and
+          <filename>hbase-env.sh</filename> -- found in the HBase <filename>conf</filename>
+        directory might look like.</para>
+
+      <section
+        xml:id="hbase_site">
+        <title><filename>hbase-site.xml</filename></title>
+
+        <programlisting language="bourne">
+<![CDATA[
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<configuration>
+  <property>
+    <name>hbase.zookeeper.quorum</name>
+    <value>example1,example2,example3</value>
+    <description>The directory shared by RegionServers.
+    </description>
+  </property>
+  <property>
+    <name>hbase.zookeeper.property.dataDir</name>
+    <value>/export/zookeeper</value>
+    <description>Property from ZooKeeper config zoo.cfg.
+    The directory where the snapshot is stored.
+    </description>
+  </property>
+  <property>
+    <name>hbase.rootdir</name>
+    <value>hdfs://example0:8020/hbase</value>
+    <description>The directory shared by RegionServers.
+    </description>
+  </property>
+  <property>
+    <name>hbase.cluster.distributed</name>
+    <value>true</value>
+    <description>The mode the cluster will be in. Possible values are
+      false: standalone and pseudo-distributed setups with managed Zookeeper
+      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
+    </description>
+  </property>
+</configuration>
+]]>
+        </programlisting>
+      </section>
+
+      <section
+        xml:id="regionservers">
+        <title><filename>regionservers</filename></title>
+
+        <para>In this file you list the nodes that will run RegionServers. In our case, these nodes
+          are <varname>example1</varname>-<varname>example9</varname>. </para>
+
+        <programlisting>
+example1
+example2
+example3
+example4
+example5
+example6
+example7
+example8
+example9
+        </programlisting>
+      </section>
+
+      <section
+        xml:id="hbase_env">
+        <title><filename>hbase-env.sh</filename></title>
+
+        <para>The following lines in the <filename>hbase-env.sh</filename> file show how to set the
+            <envar>JAVA_HOME</envar> environment variable (required for HBase 0.98.5 and newer) and
+          set the heap to 4 GB (rather than the default value of 1 GB). If you copy and paste this
+          example, be sure to adjust the <envar>JAVA_HOME</envar> to suit your environment.</para>
+
+        <screen language="bourne">
+# The java implementation to use.
+export JAVA_HOME=/usr/java/jdk1.7.0/          
+
+# The maximum amount of heap to use, in MB. Default is 1000.
+export HBASE_HEAPSIZE=4096
+        </screen>
+
+        <para>Use <command>rsync</command> to copy the content of the <filename>conf</filename>
+          directory to all nodes of the cluster.</para>
+      </section>
+    </section>
+  </section>
+  <!-- example config -->
+
+
+  <section
+    xml:id="important_configurations">
+    <title>The Important Configurations</title>
+    <para>Below we list what the <emphasis>important</emphasis> Configurations. We've divided this
+      section into required configuration and worth-a-look recommended configs. </para>
+
+
+    <section
+      xml:id="required_configuration">
+      <title>Required Configurations</title>
+      <para>Review the <xref
+          linkend="os" /> and <xref
+          linkend="hadoop" /> sections. </para>
+      <section
+        xml:id="big.cluster.config">
+        <title>Big Cluster Configurations</title>
+        <para>If a cluster with a lot of regions, it is possible if an eager beaver regionserver
+          checks in soon after master start while all the rest in the cluster are laggardly, this
+          first server to checkin will be assigned all regions. If lots of regions, this first
+          server could buckle under the load. To prevent the above scenario happening up the
+            <varname>hbase.master.wait.on.regionservers.mintostart</varname> from its default value
+          of 1. See <link
+            xlink:href="https://issues.apache.org/jira/browse/HBASE-6389">HBASE-6389 Modify the
+            conditions to ensure that Master waits for sufficient number of Region Servers before
+            starting region assignments</link> for more detail. </para>
+      </section>
+      <section
+        xml:id="backup.master.fail.fast">
+        <title>If a backup Master, making primary Master fail fast</title>
+        <para>If the primary Master loses its connection with ZooKeeper, it will fall into a loop
+          where it keeps trying to reconnect. Disable this functionality if you are running more
+          than one Master: i.e. a backup Master. Failing to do so, the dying Master may continue to
+          receive RPCs though another Master has assumed the role of primary. See the configuration <xref
+            linkend="fail.fast.expired.active.master" />. </para>
+      </section>
+    </section>
+
+    <section
+      xml:id="recommended_configurations">
+      <title>Recommended Configurations</title>
+      <section
+        xml:id="recommended_configurations.zk">
+        <title>ZooKeeper Configuration</title>
+        <section
+          xml:id="sect.zookeeper.session.timeout">
+          <title><varname>zookeeper.session.timeout</varname></title>
+          <para>The default timeout is three minutes (specified in milliseconds). This means that if
+            a server crashes, it will be three minutes before the Master notices the crash and
+            starts recovery. You might like to tune the timeout down to a minute or even less so the
+            Master notices failures the sooner. Before changing this value, be sure you have your
+            JVM garbage collection configuration under control otherwise, a long garbage collection
+            that lasts beyond the ZooKeeper session timeout will take out your RegionServer (You
+            might be fine with this -- you probably want recovery to start on the server if a
+            RegionServer has been in GC for a long period of time).</para>
+
+          <para>To change this configuration, edit <filename>hbase-site.xml</filename>, copy the
+            changed file around the cluster and restart.</para>
+
+          <para>We set this value high to save our having to field noob questions up on the mailing
+            lists asking why a RegionServer went down during a massive import. The usual cause is
+            that their JVM is untuned and they are running into long GC pauses. Our thinking is that
+            while users are getting familiar with HBase, we'd save them having to know all of its
+            intricacies. Later when they've built some confidence, then they can play with
+            configuration such as this. </para>
+        </section>
+        <section
+          xml:id="zookeeper.instances">
+          <title>Number of ZooKeeper Instances</title>
+          <para>See <xref
+              linkend="zookeeper" />. </para>
+        </section>
+      </section>
+      <section
+        xml:id="recommended.configurations.hdfs">
+        <title>HDFS Configurations</title>
+        <section
+          xml:id="dfs.datanode.failed.volumes.tolerated">
+          <title>dfs.datanode.failed.volumes.tolerated</title>
+          <para>This is the "...number of volumes that are allowed to fail before a datanode stops
+            offering service. By default any volume failure will cause a datanode to shutdown" from
+            the <filename>hdfs-default.xml</filename> description. If you have > three or four
+            disks, you might want to set this to 1 or if you have many disks, two or more. </para>
+        </section>
+      </section>
+      <section
+        xml:id="hbase.regionserver.handler.count-description">
+        <title><varname>hbase.regionserver.handler.count</varname></title>
+        <para> This setting defines the number of threads that are kept open to answer incoming
+          requests to user tables. The rule of thumb is to keep this number low when the payload per
+          request approaches the MB (big puts, scans using a large cache) and high when the payload
+          is small (gets, small puts, ICVs, deletes). The total size of the queries in progress is
+          limited by the setting "hbase.ipc.server.max.callqueue.size". </para>
+        <para> It is safe to set that number to the maximum number of incoming clients if their
+          payload is small, the typical example being a cluster that serves a website since puts
+          aren't typically buffered and most of the operations are gets. </para>
+        <para> The reason why it is dangerous to keep this setting high is that the aggregate size
+          of all the puts that are currently happening in a region server may impose too much
+          pressure on its memory, or even trigger an OutOfMemoryError. A region server running on
+          low memory will trigger its JVM's garbage collector to run more frequently up to a point
+          where GC pauses become noticeable (the reason being that all the memory used to keep all
+          the requests' payloads cannot be trashed, no matter how hard the garbage collector tries).
+          After some time, the overall cluster throughput is affected since every request that hits
+          that region server will take longer, which exacerbates the problem even more. </para>
+        <para>You can get a sense of whether you have too little or too many handlers by <xref
+            linkend="rpc.logging" /> on an individual RegionServer then tailing its logs (Queued
+          requests consume memory). </para>
+      </section>
+      <section
+        xml:id="big_memory">
+        <title>Configuration for large memory machines</title>
+        <para> HBase ships with a reasonable, conservative configuration that will work on nearly
+          all machine types that people might want to test with. If you have larger machines --
+          HBase has 8G and larger heap -- you might the following configuration options helpful.
+          TODO. </para>
+
+      </section>
+
+      <section
+        xml:id="config.compression">
+        <title>Compression</title>
+        <para>You should consider enabling ColumnFamily compression. There are several options that
+          are near-frictionless and in most all cases boost performance by reducing the size of
+          StoreFiles and thus reducing I/O. </para>
+        <para>See <xref
+            linkend="compression" /> for more information.</para>
+      </section>
+      <section
+        xml:id="config.wals">
+        <title>Configuring the size and number of WAL files</title>
+        <para>HBase uses <xref
+            linkend="wal" /> to recover the memstore data that has not been flushed to disk in case
+          of an RS failure. These WAL files should be configured to be slightly smaller than HDFS
+          block (by default, HDFS block is 64Mb and WAL file is ~60Mb).</para>
+        <para>HBase also has a limit on number of WAL files, designed to ensure there's never too
+          much data that needs to be replayed during recovery. This limit needs to be set according
+          to memstore configuration, so that all the necessary data would fit. It is recommended to
+          allocated enough WAL files to store at least that much data (when all memstores are close
+          to full). For example, with 16Gb RS heap, default memstore settings (0.4), and default WAL
+          file size (~60Mb), 16Gb*0.4/60, the starting point for WAL file count is ~109. However, as
+          all memstores are not expected to be full all the time, less WAL files can be
+          allocated.</para>
+      </section>
+      <section
+        xml:id="disable.splitting">
+        <title>Managed Splitting</title>
+        <para>HBase generally handles splitting your regions, based upon the settings in your
+            <filename>hbase-default.xml</filename> and <filename>hbase-site.xml</filename>
+          configuration files. Important settings include
+            <varname>hbase.regionserver.region.split.policy</varname>,
+            <varname>hbase.hregion.max.filesize</varname>,
+            <varname>hbase.regionserver.regionSplitLimit</varname>. A simplistic view of splitting
+          is that when a region grows to <varname>hbase.hregion.max.filesize</varname>, it is split.
+          For most use patterns, most of the time, you should use automatic splitting. See <xref
+            linkend="manual_region_splitting_decisions"/> for more information about manual region
+          splitting.</para>
+        <para>Instead of allowing HBase to split your regions automatically, you can choose to
+          manage the splitting yourself. This feature was added in HBase 0.90.0. Manually managing
+          splits works if you know your keyspace well, otherwise let HBase figure where to split for you.
+          Manual splitting can mitigate region creation and movement under load. It also makes it so
+          region boundaries are known and invariant (if you disable region splitting). If you use manual
+          splits, it is easier doing staggered, time-based major compactions spread out your network IO
+          load.</para>
+
+        <formalpara>
+          <title>Disable Automatic Splitting</title>
+          <para>To disable automatic splitting, set <varname>hbase.hregion.max.filesize</varname> to
+            a very large value, such as <literal>100 GB</literal> It is not recommended to set it to
+            its absolute maximum value of <literal>Long.MAX_VALUE</literal>.</para>
+        </formalpara>
+        <note>
+          <title>Automatic Splitting Is Recommended</title>
+          <para>If you disable automatic splits to diagnose a problem or during a period of fast
+            data growth, it is recommended to re-enable them when your situation becomes more
+            stable. The potential benefits of managing region splits yourself are not
+            undisputed.</para>
+        </note>
+        <formalpara>
+          <title>Determine the Optimal Number of Pre-Split Regions</title>
+          <para>The optimal number of pre-split regions depends on your application and environment.
+            A good rule of thumb is to start with 10 pre-split regions per server and watch as data
+            grows over time. It is better to err on the side of too few regions and perform rolling
+            splits later. The optimal number of regions depends upon the largest StoreFile in your
+            region. The size of the largest StoreFile will increase with time if the amount of data
+            grows. The goal is for the largest region to be just large enough that the compaction
+            selection algorithm only compacts it during a timed major compaction. Otherwise, the
+            cluster can be prone to compaction storms where a large number of regions under
+            compaction at the same time. It is important to understand that the data growth causes
+            compaction storms, and not the manual split decision.</para>
+        </formalpara>
+        <para>If the regions are split into too many large regions, you can increase the major
+          compaction interval by configuring <varname>HConstants.MAJOR_COMPACTION_PERIOD</varname>.
+          HBase 0.90 introduced <classname>org.apache.hadoop.hbase.util.RegionSplitter</classname>,
+          which provides a network-IO-safe rolling split of all regions.</para>
+      </section>
+      <section
+        xml:id="managed.compactions">
+        <title>Managed Compactions</title>
+        <para>By default, major compactions are scheduled to run once in a 7-day period. Prior to HBase 0.96.x, major
+          compactions were scheduled to happen once per day by default.</para>
+        <para>If you need to control exactly when and how often major compaction runs, you can
+          disable managed major compactions. See the entry for
+            <varname>hbase.hregion.majorcompaction</varname> in the <xref
+            linkend="compaction.parameters" /> table for details.</para>
+        <warning>
+          <title>Do Not Disable Major Compactions</title>
+          <para>Major compactions are absolutely necessary for StoreFile clean-up. Do not disable
+            them altogether. You can run major compactions manually via the HBase shell or via the <link
+              xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29">HBaseAdmin
+              API</link>.</para>
+        </warning>        
+        <para>For more information about compactions and the compaction file selection process, see <xref
+            linkend="compaction" /></para>
+      </section>
+
+      <section
+        xml:id="spec.ex">
+        <title>Speculative Execution</title>
+        <para>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it
+          is generally advised to turn off Speculative Execution at a system-level unless you need
+          it for a specific case, where it can be configured per-job. Set the properties
+            <varname>mapreduce.map.speculative</varname> and
+            <varname>mapreduce.reduce.speculative</varname> to false. </para>
+      </section>
+    </section>
+      <section xml:id="other_configuration"><title>Other Configurations</title>
+         <section xml:id="balancer_config"><title>Balancer</title>
+           <para>The balancer is a periodic operation which is run on the master to redistribute regions on the cluster.  It is configured via
+           <varname>hbase.balancer.period</varname> and defaults to 300000 (5 minutes). </para>
+           <para>See <xref linkend="master.processes.loadbalancer" /> for more information on the LoadBalancer.
+           </para>
+         </section>
+        <section xml:id="disabling.blockcache"><title>Disabling Blockcache</title>
+          <para>Do not turn off block cache (You'd do it by setting <varname>hbase.block.cache.size</varname> to zero).
+          Currently we do not do well if you do this because the regionserver will spend all its time loading hfile
+          indices over and over again.  If your working set it such that block cache does you no good, at least
+          size the block cache such that hfile indices will stay up in the cache (you can get a rough idea
+          on the size you need by surveying regionserver UIs; you'll see index block size accounted near the
+          top of the webpage).</para>
+        </section>
+    <section xml:id="nagles">
+      <title><link xlink:href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's</link> or the small package problem</title>
+      <para>If a big 40ms or so occasional delay is seen in operations against HBase,
+      try the Nagles' setting.  For example, see the user mailing list thread,
+      <link xlink:href="http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&amp;subj=Re+Inconsistent+scan+performance+with+caching+set+to+1">Inconsistent scan performance with caching set to 1</link>
+      and the issue cited therein where setting notcpdelay improved scan speeds.  You might also
+      see the graphs on the tail of <link xlink:href="https://issues.apache.org/jira/browse/HBASE-7008">HBASE-7008 Set scanner caching to a better default</link>
+      where our Lars Hofhansl tries various data sizes w/ Nagle's on and off measuring the effect.</para>
+    </section>
+    <section xml:id="mttr">
+      <title>Better Mean Time to Recover (MTTR)</title>
+      <para>This section is about configurations that will make servers come back faster after a fail.
+          See the Deveraj Das an Nicolas Liochon blog post
+          <link xlink:href="http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/">Introduction to HBase Mean Time to Recover (MTTR)</link>
+          for a brief introduction.</para>
+      <para>The issue <link xlink:href="https://issues.apache.org/jira/browse/HBASE-8389">HBASE-8354 forces Namenode into loop with lease recovery requests</link>
+          is messy but has a bunch of good discussion toward the end on low timeouts and how to effect faster recovery including citation of fixes
+          added to HDFS.  Read the Varun Sharma comments.  The below suggested configurations are Varun's suggestions distilled and tested.  Make sure you are
+          running on a late-version HDFS so you have the fixes he refers too and himself adds to HDFS that help HBase MTTR
+          (e.g. HDFS-3703, HDFS-3712, and HDFS-4791 -- hadoop 2 for sure has them and late hadoop 1 has some).
+          Set the following in the RegionServer.</para>
+      <programlisting language="xml">
+<![CDATA[<property>
+<property>
+    <name>hbase.lease.recovery.dfs.timeout</name>
+    <value>23000</value>
+    <description>How much time we allow elapse between calls to recover lease.
+    Should be larger than the dfs timeout.</description>
+</property>
+<property>
+    <name>dfs.client.socket-timeout</name>
+    <value>10000</value>
+    <description>Down the DFS timeout from 60 to 10 seconds.</description>
+</property>
+]]></programlisting>
+
+        <para>And on the namenode/datanode side, set the following to enable 'staleness' introduced
+          in HDFS-3703, HDFS-3912. </para>
+        <programlisting language="xml"><![CDATA[
+<property>
+    <name>dfs.client.socket-timeout</name>
+    <value>10000</value>
+    <description>Down the DFS timeout from 60 to 10 seconds.</description>
+</property>
+<property>
+    <name>dfs.datanode.socket.write.timeout</name>
+    <value>10000</value>
+    <description>Down the DFS timeout from 8 * 60 to 10 seconds.</description>
+</property>
+<property>
+    <name>ipc.client.connect.timeout</name>
+    <value>3000</value>
+    <description>Down from 60 seconds to 3.</description>
+</property>
+<property>
+    <name>ipc.client.connect.max.retries.on.timeouts</name>
+    <value>2</value>
+    <description>Down from 45 seconds to 3 (2 == 3 retries).</description>
+</property>
+<property>
+    <name>dfs.namenode.avoid.read.stale.datanode</name>
+    <value>true</value>
+    <description>Enable stale state in hdfs</description>
+</property>
+<property>
+    <name>dfs.namenode.stale.datanode.interval</name>
+    <value>20000</value>
+    <description>Down from default 30 seconds</description>
+</property>
+<property>
+    <name>dfs.namenode.avoid.write.stale.datanode</name>
+    <value>true</value>
+    <description>Enable stale state in hdfs</description>
+</property>
+]]></programlisting>
+      </section>
+
+      <section
+        xml:id="JMX_config">
+        <title>JMX</title>
+        <para>JMX(Java Management Extensions) provides built-in instrumentation that enables you
+          to monitor and manage the Java VM. To enable monitoring and management from remote
+          systems, you need to set system property com.sun.management.jmxremote.port(the port
+          number through which you want to enable JMX RMI connections) when you start the Java VM.
+          See <link
+            xlink:href="http://docs.oracle.com/javase/6/docs/technotes/guides/management/agent.html">
+          official document</link> for more information. Historically, besides above port mentioned,
+          JMX opens 2 additional random TCP listening ports, which could lead to port conflict
+          problem.(See <link
+            xlink:href="https://issues.apache.org/jira/browse/HBASE-10289">HBASE-10289</link>
+          for details)
+        </para>
+        <para>As an alternative, You can use the coprocessor-based JMX implementation provided
+          by HBase. To enable it in 0.99 or above, add below property in
+          <filename>hbase-site.xml</filename>:
+        <programlisting language="xml"><![CDATA[
+<property>
+    <name>hbase.coprocessor.regionserver.classes</name>
+    <value>org.apache.hadoop.hbase.JMXListener</value>
+</property>
+]]></programlisting>
+          NOTE: DO NOT set com.sun.management.jmxremote.port for Java VM at the same time.
+        </para>
+        <para>Currently it supports Master and RegionServer Java VM. The reason why you only
+          configure coprocessor for 'regionserver' is that, starting from HBase 0.99,
+          a Master IS also a RegionServer. (See <link
+            xlink:href="https://issues.apache.org/jira/browse/HBASE-10569">HBASE-10569</link>
+          for more information.)
+          By default, the JMX listens on TCP port 10102, you can further configure the port
+          using below properties:
+
+        <programlisting language="xml"><![CDATA[
+<property>
+    <name>regionserver.rmi.registry.port</name>
+    <value>61130</value>
+</property>
+<property>
+    <name>regionserver.rmi.connector.port</name>
+    <value>61140</value>
+</property>
+]]></programlisting>
+          The registry port can be shared with connector port in most cases, so you only
+          need to configure regionserver.rmi.registry.port. However if you want to use SSL
+          communication, the 2 ports must be configured to different values.
+        </para>
+
+        <para>By default the password authentication and SSL communication is disabled.
+          To enable password authentication, you need to update <filename>hbase-env.sh</filename>
+          like below:
+      <screen language="bourne">
+export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.authenticate=true                  \
+                       -Dcom.sun.management.jmxremote.password.file=your_password_file   \
+                       -Dcom.sun.management.jmxremote.access.file=your_access_file"
+
+export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
+export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
+      </screen>
+          See example password/access file under $JRE_HOME/lib/management.
+        </para>
+
+        <para>To enable SSL communication with password authentication, follow below steps:
+      <screen language="bourne">
+#1. generate a key pair, stored in myKeyStore
+keytool -genkey -alias jconsole -keystore myKeyStore
+
+#2. export it to file jconsole.cert
+keytool -export -alias jconsole -keystore myKeyStore -file jconsole.cert
+
+#3. copy jconsole.cert to jconsole client machine, import it to jconsoleKeyStore
+keytool -import -alias jconsole -keystore jconsoleKeyStore -file jconsole.cert
+      </screen>
+          And then update <filename>hbase-env.sh</filename> like below:
+      <screen language="bourne">
+export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=true                         \
+                       -Djavax.net.ssl.keyStore=/home/tianq/myKeyStore                 \
+                       -Djavax.net.ssl.keyStorePassword=your_password_in_step_1       \
+                       -Dcom.sun.management.jmxremote.authenticate=true                \
+                       -Dcom.sun.management.jmxremote.password.file=your_password file \
+                       -Dcom.sun.management.jmxremote.access.file=your_access_file"
+
+export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
+export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
+      </screen>
+
+          Finally start jconsole on client using the key store:
+      <screen language="bourne">
+jconsole -J-Djavax.net.ssl.trustStore=/home/tianq/jconsoleKeyStore
+      </screen>
+        </para>
+        <para>NOTE: for HBase 0.98, To enable the HBase JMX implementation on Master, you also
+          need to add below property in <filename>hbase-site.xml</filename>:
+        <programlisting language="xml"><![CDATA[
+<property>
+    <name>hbase.coprocessor.master.classes</name>
+    <value>org.apache.hadoop.hbase.JMXListener</value>
+</property>
+]]></programlisting>
+          The corresponding properties for port configuration are master.rmi.registry.port
+          (by default 10101) and master.rmi.connector.port(by default the same as registry.port)
+        </para>
+    </section>
+
+   </section>
+
+  </section>
+  <!--  important config -->
+  <section xml:id="dyn_config">
+    <title>Dynamic Configuration</title>
+    <subtitle>Changing Configuration Without Restarting Servers</subtitle>
+    <para>Since HBase 1.0.0, it is possible to change a subset of the configuration without
+      requiring a server restart. In the hbase shell, there are new operators,
+      <command>update_config</command> and <command>update_all_config</command> that
+      will prompt a server or all servers to reload configuration.</para>
+    <para>Only a subset of all configurations can currently be changed in the running server.
+      Here is an incomplete list:
+      <property>hbase.regionserver.thread.compaction.large</property>,
+      <property>hbase.regionserver.thread.compaction.small</property>,
+      <property>hbase.regionserver.thread.split</property>,
+      <property>hbase.regionserver.thread.merge</property>, as well as compaction
+      policy and configurations and adjustment to offpeak hours.
+      For the full list consult the patch attached to 
+      <link xlink:href="https://issues.apache.org/jira/browse/HBASE-12147">HBASE-12147 Porting Online Config Change from 89-fb</link>.
+    </para>
+
+  </section>
+</chapter>
diff --git a/src/main/docbkx/troubleshooting.xml b/src/main/docbkx/troubleshooting.xml
new file mode 100644
index 0000000..7b106d6
--- /dev/null
+++ b/src/main/docbkx/troubleshooting.xml
@@ -0,0 +1,1700 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<chapter
+  version="5.0"
+  xml:id="trouble"
+  xmlns="http://docbook.org/ns/docbook"
+  xmlns:xlink="http://www.w3.org/1999/xlink"
+  xmlns:xi="http://www.w3.org/2001/XInclude"
+  xmlns:svg="http://www.w3.org/2000/svg"
+  xmlns:m="http://www.w3.org/1998/Math/MathML"
+  xmlns:html="http://www.w3.org/1999/xhtml"
+  xmlns:db="http://docbook.org/ns/docbook">
+  <!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+  <title>Troubleshooting and Debugging Apache HBase</title>
+  <section
+    xml:id="trouble.general">
+    <title>General Guidelines</title>
+    <para> Always start with the master log (TODO: Which lines?). Normally it’s just printing the
+      same lines over and over again. If not, then there’s an issue. Google or <link
+        xlink:href="http://search-hadoop.com">search-hadoop.com</link> should return some hits for
+      those exceptions you’re seeing. </para>
+    <para> An error rarely comes alone in Apache HBase, usually when something gets screwed up what
+      will follow may be hundreds of exceptions and stack traces coming from all over the place. The
+      best way to approach this type of problem is to walk the log up to where it all began, for
+      example one trick with RegionServers is that they will print some metrics when aborting so
+      grepping for <emphasis>Dump</emphasis> should get you around the start of the problem. </para>
+    <para> RegionServer suicides are “normal”, as this is what they do when something goes wrong.
+      For example, if ulimit and max transfer threads (the two most important initial settings, see
+      <xref linkend="ulimit" /> and <xref linkend="dfs.datanode.max.transfer.threads" />) aren’t
+      changed, it will make it impossible at some point for DataNodes
+      to create new threads that from the HBase point of view is seen as if HDFS was gone. Think
+      about what would happen if your MySQL database was suddenly unable to access files on your
+      local file system, well it’s the same with HBase and HDFS. Another very common reason to see
+      RegionServers committing seppuku is when they enter prolonged garbage collection pauses that
+      last longer than the default ZooKeeper session timeout. For more information on GC pauses, see
+      the <link
+        xlink:href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/">3
+        part blog post</link> by Todd Lipcon and <xref
+        linkend="gcpause" /> above. </para>
+  </section>
+  <section
+    xml:id="trouble.log">
+    <title>Logs</title>
+    <para> The key process logs are as follows... (replace &lt;user&gt; with the user that started
+      the service, and &lt;hostname&gt; for the machine name) </para>
+    <para> NameNode:
+        <filename>$HADOOP_HOME/logs/hadoop-&lt;user&gt;-namenode-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> DataNode:
+        <filename>$HADOOP_HOME/logs/hadoop-&lt;user&gt;-datanode-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> JobTracker:
+        <filename>$HADOOP_HOME/logs/hadoop-&lt;user&gt;-jobtracker-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> TaskTracker:
+        <filename>$HADOOP_HOME/logs/hadoop-&lt;user&gt;-tasktracker-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> HMaster:
+        <filename>$HBASE_HOME/logs/hbase-&lt;user&gt;-master-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> RegionServer:
+        <filename>$HBASE_HOME/logs/hbase-&lt;user&gt;-regionserver-&lt;hostname&gt;.log</filename>
+    </para>
+    <para> ZooKeeper: <filename>TODO</filename>
+    </para>
+    <section
+      xml:id="trouble.log.locations">
+      <title>Log Locations</title>
+      <para>For stand-alone deployments the logs are obviously going to be on a single machine,
+        however this is a development configuration only. Production deployments need to run on a
+        cluster.</para>
+      <section
+        xml:id="trouble.log.locations.namenode">
+        <title>NameNode</title>
+        <para>The NameNode log is on the NameNode server. The HBase Master is typically run on the
+          NameNode server, and well as ZooKeeper.</para>
+        <para>For smaller clusters the JobTracker is typically run on the NameNode server as
+          well.</para>
+      </section>
+      <section
+        xml:id="trouble.log.locations.datanode">
+        <title>DataNode</title>
+        <para>Each DataNode server will have a DataNode log for HDFS, as well as a RegionServer log
+          for HBase.</para>
+        <para>Additionally, each DataNode server will also have a TaskTracker log for MapReduce task
+          execution.</para>
+      </section>
+    </section>
+    <section
+      xml:id="trouble.log.levels">
+      <title>Log Levels</title>
+      <section
+        xml:id="rpc.logging">
+        <title>Enabling RPC-level logging</title>
+        <para>Enabling the RPC-level logging on a RegionServer can often given insight on timings at
+          the server. Once enabled, the amount of log spewed is voluminous. It is not recommended
+          that you leave this logging on for more than short bursts of time. To enable RPC-level
+          logging, browse to the RegionServer UI and click on <emphasis>Log Level</emphasis>. Set
+          the log level to <varname>DEBUG</varname> for the package
+            <classname>org.apache.hadoop.ipc</classname> (Thats right, for
+            <classname>hadoop.ipc</classname>, NOT, <classname>hbase.ipc</classname>). Then tail the
+          RegionServers log. Analyze.</para>
+        <para>To disable, set the logging level back to <varname>INFO</varname> level. </para>
+      </section>
+    </section>
+    <section
+      xml:id="trouble.log.gc">
+      <title>JVM Garbage Collection Logs</title>
+      <para>HBase is memory intensive, and using the default GC you can see long pauses in all
+        threads including the <emphasis>Juliet Pause</emphasis> aka "GC of Death". To help debug
+        this or confirm this is happening GC logging can be turned on in the Java virtual machine. </para>
+      <para> To enable, in <filename>hbase-env.sh</filename>, uncomment one of the below lines
+        :</para>
+      <programlisting language="bourne">
+# This enables basic gc logging to the .out file.
+# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
+
+# This enables basic gc logging to its own file.
+# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"
+
+# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
+# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
+
+# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR.
+          </programlisting>
+      <para> At this point you should see logs like so:</para>
+      <programlisting>
+64898.952: [GC [1 CMS-initial-mark: 2811538K(3055704K)] 2812179K(3061272K), 0.0007360 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
+64898.953: [CMS-concurrent-mark-start]
+64898.971: [GC 64898.971: [ParNew: 5567K->576K(5568K), 0.0101110 secs] 2817105K->2812715K(3061272K), 0.0102200 secs] [Times: user=0.07 sys=0.00, real=0.01 secs]
+          </programlisting>
+      <para> In this section, the first line indicates a 0.0007360 second pause for the CMS to
+        initially mark. This pauses the entire VM, all threads for that period of time. </para>
+      <para> The third line indicates a "minor GC", which pauses the VM for 0.0101110 seconds - aka
+        10 milliseconds. It has reduced the "ParNew" from about 5.5m to 576k. Later on in this cycle
+        we see:</para>
+      <programlisting>
+64901.445: [CMS-concurrent-mark: 1.542/2.492 secs] [Times: user=10.49 sys=0.33, real=2.49 secs]
+64901.445: [CMS-concurrent-preclean-start]
+64901.453: [GC 64901.453: [ParNew: 5505K->573K(5568K), 0.0062440 secs] 2868746K->2864292K(3061272K), 0.0063360 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
+64901.476: [GC 64901.476: [ParNew: 5563K->575K(5568K), 0.0072510 secs] 2869283K->2864837K(3061272K), 0.0073320 secs] [Times: user=0.05 sys=0.01, real=0.01 secs]
+64901.500: [GC 64901.500: [ParNew: 5517K->573K(5568K), 0.0120390 secs] 2869780K->2865267K(3061272K), 0.0121150 secs] [Times: user=0.09 sys=0.00, real=0.01 secs]
+64901.529: [GC 64901.529: [ParNew: 5507K->569K(5568K), 0.0086240 secs] 2870200K->2865742K(3061272K), 0.0087180 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
+64901.554: [GC 64901.555: [ParNew: 5516K->575K(5568K), 0.0107130 secs] 2870689K->2866291K(3061272K), 0.0107820 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
+64901.578: [CMS-concurrent-preclean: 0.070/0.133 secs] [Times: user=0.48 sys=0.01, real=0.14 secs]
+64901.578: [CMS-concurrent-abortable-preclean-start]
+64901.584: [GC 64901.584: [ParNew: 5504K->571K(5568K), 0.0087270 secs] 2871220K->2866830K(3061272K), 0.0088220 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
+64901.609: [GC 64901.609: [ParNew: 5512K->569K(5568K), 0.0063370 secs] 2871771K->2867322K(3061272K), 0.0064230 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
+64901.615: [CMS-concurrent-abortable-preclean: 0.007/0.037 secs] [Times: user=0.13 sys=0.00, real=0.03 secs]
+64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
+64901.621: [CMS-concurrent-sweep-start]
+            </programlisting>
+      <para> The first line indicates that the CMS concurrent mark (finding garbage) has taken 2.4
+        seconds. But this is a _concurrent_ 2.4 seconds, Java has not been paused at any point in
+        time. </para>
+      <para> There are a few more minor GCs, then there is a pause at the 2nd last line:
+        <programlisting>
+64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
+            </programlisting>
+      </para>
+      <para> The pause here is 0.0049380 seconds (aka 4.9 milliseconds) to 'remark' the heap. </para>
+      <para> At this point the sweep starts, and you can watch the heap size go down:</para>
+      <programlisting>
+64901.637: [GC 64901.637: [ParNew: 5501K->569K(5568K), 0.0097350 secs] 2871958K->2867441K(3061272K), 0.0098370 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
+...  lines removed ...
+64904.936: [GC 64904.936: [ParNew: 5532K->568K(5568K), 0.0070720 secs] 1365024K->1360689K(3061272K), 0.0071930 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
+64904.953: [CMS-concurrent-sweep: 2.030/3.332 secs] [Times: user=9.57 sys=0.26, real=3.33 secs]
+            </programlisting>
+      <para>At this point, the CMS sweep took 3.332 seconds, and heap went from about ~ 2.8 GB to
+        1.3 GB (approximate). </para>
+      <para> The key points here is to keep all these pauses low. CMS pauses are always low, but if
+        your ParNew starts growing, you can see minor GC pauses approach 100ms, exceed 100ms and hit
+        as high at 400ms. </para>
+      <para> This can be due to the size of the ParNew, which should be relatively small. If your
+        ParNew is very large after running HBase for a while, in one example a ParNew was about
+        150MB, then you might have to constrain the size of ParNew (The larger it is, the longer the
+        collections take but if its too small, objects are promoted to old gen too quickly). In the
+        below we constrain new gen size to 64m. </para>
+      <para> Add the below line in <filename>hbase-env.sh</filename>:
+        <programlisting language="bourne">
+export SERVER_GC_OPTS="$SERVER_GC_OPTS -XX:NewSize=64m -XX:MaxNewSize=64m"
+            </programlisting>
+      </para>
+      <para> Similarly, to enable GC logging for client processes, uncomment one of the below lines
+        in <filename>hbase-env.sh</filename>:</para>
+      <programlisting language="bourne">
+# This enables basic gc logging to the .out file.
+# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
+
+# This enables basic gc logging to its own file.
+# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"
+
+# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
+# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"
+
+# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR .
+            </programlisting>
+      <para> For more information on GC pauses, see the <link
+          xlink:href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/">3
+          part blog post</link> by Todd Lipcon and <xref
+          linkend="gcpause" /> above. </para>
+    </section>
+  </section>
+  <section
+    xml:id="trouble.resources">
+    <title>Resources</title>
+    <section
+      xml:id="trouble.resources.searchhadoop">
+      <title>search-hadoop.com</title>
+      <para>
+        <link
+          xlink:href="http://search-hadoop.com">search-hadoop.com</link> indexes all the mailing
+        lists and is great for historical searches. Search here first when you have an issue as its
+        more than likely someone has already had your problem. </para>
+    </section>
+    <section
+      xml:id="trouble.resources.lists">
+      <title>Mailing Lists</title>
+      <para>Ask a question on the <link xlink:href="http://hbase.apache.org/mail-lists.html">Apache
+          HBase mailing lists</link>. The 'dev' mailing list is aimed at the community of developers
+        actually building Apache HBase and for features currently under development, and 'user' is
+        generally used for questions on released versions of Apache HBase. Before going to the
+        mailing list, make sure your question has not already been answered by searching the mailing
+        list archives first. Use <xref linkend="trouble.resources.searchhadoop"/>. Take some time
+        crafting your question. See <link xlink:href="http://www.mikeash.com/getting_answers.html"
+          >Getting Answers</link> for ideas on crafting good questions. A quality question that
+        includes all context and exhibits evidence the author has tried to find answers in the
+        manual and out on lists is more likely to get a prompt response. </para>
+    </section>
+    <section
+      xml:id="trouble.resources.irc">
+      <title>IRC</title>
+      <para>#hbase on irc.freenode.net</para>
+    </section>
+    <section
+      xml:id="trouble.resources.jira">
+      <title>JIRA</title>
+      <para>
+        <link
+          xlink:href="https://issues.apache.org/jira/browse/HBASE">JIRA</link> is also really
+        helpful when looking for Hadoop/HBase-specific issues. </para>
+    </section>
+  </section>
+  <section
+    xml:id="trouble.tools">
+    <title>Tools</title>
+    <section
+      xml:id="trouble.tools.builtin">
+      <title>Builtin Tools</title>
+      <section
+        xml:id="trouble.tools.builtin.webmaster">
+        <title>Master Web Interface</title>
+        <para>The Master starts a web-interface on port 60010 by default. (Up to and including 0.98
+          this was port 60010) </para>
+        <para>The Master web UI lists created tables and their definition (e.g., ColumnFamilies,
+          blocksize, etc.). Additionally, the available RegionServers in the cluster are listed
+          along with selected high-level metrics (requests, number of regions, usedHeap, maxHeap).
+          The Master web UI allows navigation to each RegionServer's web UI. </para>
+      </section>
+      <section
+        xml:id="trouble.tools.builtin.webregion">
+        <title>RegionServer Web Interface</title>
+        <para>RegionServers starts a web-interface on port 60030 by default. (Up to an including
+          0.98 this was port 60030) </para>
+        <para>The RegionServer web UI lists online regions and their start/end keys, as well as
+          point-in-time RegionServer metrics (requests, regions, storeFileIndexSize,
+          compactionQueueSize, etc.). </para>
+        <para>See <xref
+            linkend="hbase_metrics" /> for more information in metric definitions. </para>
+      </section>
+      <section
+        xml:id="trouble.tools.builtin.zkcli">
+        <title>zkcli</title>
+        <para><code>zkcli</code> is a very useful tool for investigating ZooKeeper-related issues.
+          To invoke:
+          <programlisting language="bourne">
+./hbase zkcli -server host:port &lt;cmd&gt; &lt;args&gt;
+</programlisting>
+          The commands (and arguments) are:</para>
+        <programlisting>
+	connect host:port
+	get path [watch]
+	ls path [watch]
+	set path data [version]
+	delquota [-n|-b] path
+	quit
+	printwatches on|off
+	create [-s] [-e] path data acl
+	stat path [watch]
+	close
+	ls2 path [watch]
+	history
+	listquota path
+	setAcl path acl
+	getAcl path
+	sync path
+	redo cmdno
+	addauth scheme auth
+	delete path [version]
+	setquota -n|-b val path
+</programlisting>
+      </section>
+    </section>
+    <section
+      xml:id="trouble.tools.external">
+      <title>External Tools</title>
+      <section
+        xml:id="trouble.tools.tail">
+        <title>tail</title>
+        <para>
+          <code>tail</code> is the command line tool that lets you look at the end of a file. Add
+          the “-f” option and it will refresh when new data is available. It’s useful when you are
+          wondering what’s happening, for example, when a cluster is taking a long time to shutdown
+          or startup as you can just fire a new terminal and tail the master log (and maybe a few
+          RegionServers). </para>
+      </section>
+      <section
+        xml:id="trouble.tools.top">
+        <title>top</title>
+        <para>
+          <code>top</code> is probably one of the most important tool when first trying to see
+          what’s running on a machine and how the resources are consumed. Here’s an example from
+          production system:</para>
+        <programlisting>
+top - 14:46:59 up 39 days, 11:55,  1 user,  load average: 3.75, 3.57, 3.84
+Tasks: 309 total,   1 running, 308 sleeping,   0 stopped,   0 zombie
+Cpu(s):  4.5%us,  1.6%sy,  0.0%ni, 91.7%id,  1.4%wa,  0.1%hi,  0.6%si,  0.0%st
+Mem:  24414432k total, 24296956k used,   117476k free,     7196k buffers
+Swap: 16008732k total,	14348k used, 15994384k free, 11106908k cached
+
+  PID USER  	PR  NI  VIRT  RES  SHR S %CPU %MEM	TIME+  COMMAND
+15558 hadoop	18  -2 3292m 2.4g 3556 S   79 10.4   6523:52 java
+13268 hadoop	18  -2 8967m 8.2g 4104 S   21 35.1   5170:30 java
+ 8895 hadoop	18  -2 1581m 497m 3420 S   11  2.1   4002:32 java
+…
+        </programlisting>
+        <para> Here we can see that the system load average during the last five minutes is 3.75,
+          which very roughly means that on average 3.75 threads were waiting for CPU time during
+          these 5 minutes. In general, the “perfect” utilization equals to the number of cores,
+          under that number the machine is under utilized and over that the machine is over
+          utilized. This is an important concept, see this article to understand it more: <link
+            xlink:href="http://www.linuxjournal.com/article/9001">http://www.linuxjournal.com/article/9001</link>. </para>
+        <para> Apart from load, we can see that the system is using almost all its available RAM but
+          most of it is used for the OS cache (which is good). The swap only has a few KBs in it and
+          this is wanted, high numbers would indicate swapping activity which is the nemesis of
+          performance of Java systems. Another way to detect swapping is when the load average goes
+          through the roof (although this could also be caused by things like a dying disk, among
+          others). </para>
+        <para> The list of processes isn’t super useful by default, all we know is that 3 java
+          processes are using about 111% of the CPUs. To know which is which, simply type “c” and
+          each line will be expanded. Typing “1” will give you the detail of how each CPU is used
+          instead of the average for all of them like shown here. </para>
+      </section>
+      <section
+        xml:id="trouble.tools.jps">
+        <title>jps</title>
+        <para>
+          <code>jps</code> is shipped with every JDK and gives the java process ids for the current
+          user (if root, then it gives the ids for all users). Example:</para>
+        <programlisting language="bourne">
+hadoop@sv4borg12:~$ jps
+1322 TaskTracker
+17789 HRegionServer
+27862 Child
+1158 DataNode
+25115 HQuorumPeer
+2950 Jps
+19750 ThriftServer
+18776 jmx
+        </programlisting>
+        <para>In order, we see a: </para>
+        <itemizedlist>
+          <listitem>
+            <para>Hadoop TaskTracker, manages the local Childs</para>
+          </listitem>
+          <listitem>
+            <para>HBase RegionServer, serves regions</para>
+          </listitem>
+          <listitem>
+            <para>Child, its MapReduce task, cannot tell which type exactly</para>
+          </listitem>
+          <listitem>
+            <para>Hadoop TaskTracker, manages the local Childs</para>
+          </listitem>
+          <listitem>
+            <para>Hadoop DataNode, serves blocks</para>
+          </listitem>
+          <listitem>
+            <para>HQuorumPeer, a ZooKeeper ensemble member</para>
+          </listitem>
+          <listitem>
+            <para>Jps, well… it’s the current process</para>
+          </listitem>
+          <listitem>
+            <para>ThriftServer, it’s a special one will be running only if thrift was started</para>
+          </listitem>
+          <listitem>
+            <para>jmx, this is a local process that’s part of our monitoring platform ( poorly named
+              maybe). You probably don’t have that.</para>
+          </listitem>
+        </itemizedlist>
+        <para> You can then do stuff like checking out the full command line that started the
+          process:</para>
+        <programlisting language="bourne">
+hadoop@sv4borg12:~$ ps aux | grep HRegionServer
+hadoop   17789  155 35.2 9067824 8604364 ?     S&lt;l  Mar04 9855:48 /usr/java/jdk1.6.0_14/bin/java -Xmx8000m -XX:+DoEscapeAnalysis -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:NewSize=64m -XX:MaxNewSize=64m -XX:CMSInitiatingOccupancyFraction=88 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/export1/hadoop/logs/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 -Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=/home/hadoop/hbase/conf/jmxremote.password -Dcom.sun.management.jmxremote -Dhbase.log.dir=/export1/hadoop/logs -Dhbase.log.file=hbase-hadoop-regionserver-sv4borg12.log -Dhbase.home.dir=/home/hadoop/hbase -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,DRFA -Djava.library.path=/home/hadoop/hbase/lib/native/Linux-amd64-64 -classpath /home/hadoop/hbase/bin/../conf:[many jars]:/home/hadoop/hadoop/conf org.apache.hadoop.hbase.regionserver.HRegionServer start
+        </programlisting>
+      </section>
+      <section
+        xml:id="trouble.tools.jstack">
+        <title>jstack</title>
+        <para>
+          <code>jstack</code> is one of the most important tools when trying to figure out what a
+          java process is doing apart from looking at the logs. It has to be used in conjunction
+          with jps in order to give it a process id. It shows a list of threads, each one has a
+          name, and they appear in the order that they were created (so the top ones are the most
+          recent threads). Here’s a few example: </para>
+        <para> The main thread of a RegionServer that’s waiting for something to do from the
+          master:</para>
+        <programlisting>
+"regionserver60020" prio=10 tid=0x0000000040ab4000 nid=0x45cf waiting on condition [0x00007f16b6a96000..0x00007f16b6a96a70]
+java.lang.Thread.State: TIMED_WAITING (parking)
+    at sun.misc.Unsafe.park(Native Method)
+    - parking to wait for  &lt;0x00007f16cd5c2f30&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
+    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
+    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
+    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:395)
+    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:647)
+    at java.lang.Thread.run(Thread.java:619)
+
+    The MemStore flusher thread that is currently flushing to a file:
+"regionserver60020.cacheFlusher" daemon prio=10 tid=0x0000000040f4e000 nid=0x45eb in Object.wait() [0x00007f16b5b86000..0x00007f16b5b87af0]
+java.lang.Thread.State: WAITING (on object monitor)
+    at java.lang.Object.wait(Native Method)
+    at java.lang.Object.wait(Object.java:485)
+    at org.apache.hadoop.ipc.Client.call(Client.java:803)
+    - locked &lt;0x00007f16cb14b3a8&gt; (a org.apache.hadoop.ipc.Client$Call)
+    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
+    at $Proxy1.complete(Unknown Source)
+    at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
+    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
+    at java.lang.reflect.Method.invoke(Method.java:597)
+    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
+    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
+    at $Proxy1.complete(Unknown Source)
+    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3390)
+    - locked &lt;0x00007f16cb14b470&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
+    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3304)
+    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
+    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
+    at org.apache.hadoop.hbase.io.hfile.HFile$Writer.close(HFile.java:650)
+    at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:853)
+    at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:467)
+    - locked &lt;0x00007f16d00e6f08&gt; (a java.lang.Object)
+    at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:427)
+    at org.apache.hadoop.hbase.regionserver.Store.access$100(Store.java:80)
+    at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:1359)
+    at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:907)
+    at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:834)
+    at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:786)
+    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:250)
+    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:224)
+    at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)
+        </programlisting>
+        <para> A handler thread that’s waiting for stuff to do (like put, delete, scan, etc):</para>
+        <programlisting>
+"IPC Server handler 16 on 60020" daemon prio=10 tid=0x00007f16b011d800 nid=0x4a5e waiting on condition [0x00007f16afefd000..0x00007f16afefd9f0]
+   java.lang.Thread.State: WAITING (parking)
+        	at sun.misc.Unsafe.park(Native Method)
+        	- parking to wait for  &lt;0x00007f16cd3f8dd8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
+        	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
+        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
+        	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
+        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1013)
+        </programlisting>
+        <para> And one that’s busy doing an increment of a counter (it’s in the phase where it’s
+          trying to create a scanner in order to read the last value):</para>
+        <programlisting>
+"IPC Server handler 66 on 60020" daemon prio=10 tid=0x00007f16b006e800 nid=0x4a90 runnable [0x00007f16acb77000..0x00007f16acb77cf0]
+   java.lang.Thread.State: RUNNABLE
+        	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.&lt;init&gt;(KeyValueHeap.java:56)
+        	at org.apache.hadoop.hbase.regionserver.StoreScanner.&lt;init&gt;(StoreScanner.java:79)
+        	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1202)
+        	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.&lt;init&gt;(HRegion.java:2209)
+        	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateInternalScanner(HRegion.java:1063)
+        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1055)
+        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1039)
+        	at org.apache.hadoop.hbase.regionserver.HRegion.getLastIncrement(HRegion.java:2875)
+        	at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:2978)
+        	at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2433)
+        	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
+        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
+        	at java.lang.reflect.Method.invoke(Method.java:597)
+        	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:560)
+        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1027)
+        </programlisting>
+        <para> A thread that receives data from HDFS:</para>
+        <programlisting>
+"IPC Client (47) connection to sv4borg9/10.4.24.40:9000 from hadoop" daemon prio=10 tid=0x00007f16a02d0000 nid=0x4fa3 runnable [0x00007f16b517d000..0x00007f16b517dbf0]
+   java.lang.Thread.State: RUNNABLE
+        	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
+        	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
+        	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
+        	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
+        	- locked &lt;0x00007f17d5b68c00&gt; (a sun.nio.ch.Util$1)
+        	- locked &lt;0x00007f17d5b68be8&gt; (a java.util.Collections$UnmodifiableSet)
+        	- locked &lt;0x00007f1877959b50&gt; (a sun.nio.ch.EPollSelectorImpl)
+        	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
+        	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:332)
+        	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
+        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
+        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
+        	at java.io.FilterInputStream.read(FilterInputStream.java:116)
+        	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:304)
+        	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
+        	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
+        	- locked &lt;0x00007f1808539178&gt; (a java.io.BufferedInputStream)
+        	at java.io.DataInputStream.readInt(DataInputStream.java:370)
+        	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:569)
+        	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:477)
+          </programlisting>
+        <para> And here is a master trying to recover a lease after a RegionServer died:</para>
+        <programlisting>
+"LeaseChecker" daemon prio=10 tid=0x00000000407ef800 nid=0x76cd waiting on condition [0x00007f6d0eae2000..0x00007f6d0eae2a70]
+--
+   java.lang.Thread.State: WAITING (on object monitor)
+        	at java.lang.Object.wait(Native Method)
+        	at java.lang.Object.wait(Object.java:485)
+        	at org.apache.hadoop.ipc.Client.call(Client.java:726)
+        	- locked &lt;0x00007f6d1cd28f80&gt; (a org.apache.hadoop.ipc.Client$Call)
+        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
+        	at $Proxy1.recoverBlock(Unknown Source)
+        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2636)
+        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&lt;init&gt;(DFSClient.java:2832)
+        	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:529)
+        	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:186)
+        	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:530)
+        	at org.apache.hadoop.hbase.util.FSUtils.recoverFileLease(FSUtils.java:619)
+        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1322)
+        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
+        	at org.apache.hadoop.hbase.master.HMaster.splitLogAfterStartup(HMaster.java:648)
+        	at org.apache.hadoop.hbase.master.HMaster.joinCluster(HMaster.java:572)
+        	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:503)
+          </programlisting>
+      </section>
+      <section
+        xml:id="trouble.tools.opentsdb">
+        <title>OpenTSDB</title>
+        <para>
+          <link
+            xlink:href="http://opentsdb.net">OpenTSDB</link> is an excellent alternative to Ganglia
+          as it uses Apache HBase to store all the time series and doesn’t have to downsample.
+          Monitoring your own HBase cluster that hosts OpenTSDB is a good exercise. </para>
+        <para> Here’s an example of a cluster that’s suffering from hundreds of compactions launched
+          almost all around the same time, which severely affects the IO performance: (TODO: insert
+          graph plotting compactionQueueSize) </para>
+        <para> It’s a good practice to build dashboards with all the important graphs per machine
+          and per cluster so that debugging issues can be done with a single quick look. For
+          example, at StumbleUpon there’s one dashboard per cluster with the most important metrics
+          from both the OS and Apache HBase. You can then go down at the machine level and get even
+          more detailed metrics. </para>
+      </section>
+      <section
+        xml:id="trouble.tools.clustersshtop">
+        <title>clusterssh+top</title>
+        <para> clusterssh+top, it’s like a poor man’s monitoring system and it can be quite useful
+          when you have only a few machines as it’s very easy to setup. Starting clusterssh will
+          give you one terminal per machine and another terminal in which whatever you type will be
+          retyped in every window. This means that you can type “top” once and it will start it for
+          all of your machines at the same time giving you full view of the current state of your
+          cluster. You can also tail all the logs at the same time, edit files, etc. </para>
+      </section>
+    </section>
+  </section>
+
+  <section
+    xml:id="trouble.client">
+    <title>Client</title>
+    <para>For more information on the HBase client, see <xref
+        linkend="client" />. </para>
+    <section
+      xml:id="trouble.client.scantimeout">
+      <title>ScannerTimeoutException or UnknownScannerException</title>
+      <para>This is thrown if the time between RPC calls from the client to RegionServer exceeds the
+        scan timeout. For example, if <code>Scan.setCaching</code> is set to 500, then there will be
+        an RPC call to fetch the next batch of rows every 500 <code>.next()</code> calls on the
+        ResultScanner because data is being transferred in blocks of 500 rows to the client.
+        Reducing the setCaching value may be an option, but setting this value too low makes for
+        inefficient processing on numbers of rows. </para>
+      <para>See <xref
+          linkend="perf.hbase.client.caching" />. </para>
+    </section>
+    <section>
+      <title>Performance Differences in Thrift and Java APIs</title>
+      <para>Poor performance, or even <code>ScannerTimeoutExceptions</code>, can occur if
+          <code>Scan.setCaching</code> is too high, as discussed in <xref
+          linkend="trouble.client.scantimeout"/>. If the Thrift client uses the wrong caching
+        settings for a given workload, performance can suffer compared to the Java API. To set
+        caching for a given scan in the Thrift client, use the <code>scannerGetList(scannerId,
+          numRows)</code> method, where <code>numRows</code> is an integer representing the number
+        of rows to cache. In one case, it was found that reducing the cache for Thrift scans from
+        1000 to 100 increased performance to near parity with the Java API given the same
+        queries.</para>
+      <para>See also Jesse Andersen's <link xlink:href="http://blog.cloudera.com/blog/2014/04/how-to-use-the-hbase-thrift-interface-part-3-using-scans/">blog post</link> 
+        about using Scans with Thrift.</para>
+    </section>
+    <section
+      xml:id="trouble.client.lease.exception">
+      <title><classname>LeaseException</classname> when calling
+        <classname>Scanner.next</classname></title>
+      <para> In some situations clients that fetch data from a RegionServer get a LeaseException
+        instead of the usual <xref
+          linkend="trouble.client.scantimeout" />. Usually the source of the exception is
+          <classname>org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:230)</classname>
+        (line number may vary). It tends to happen in the context of a slow/freezing
+        RegionServer#next call. It can be prevented by having <varname>hbase.rpc.timeout</varname> >
+          <varname>hbase.regionserver.lease.period</varname>. Harsh J investigated the issue as part
+        of the mailing list thread <link
+          xlink:href="http://mail-archives.apache.org/mod_mbox/hbase-user/201209.mbox/%3CCAOcnVr3R-LqtKhFsk8Bhrm-YW2i9O6J6Fhjz2h7q6_sxvwd2yw%40mail.gmail.com%3E">HBase,
+          mail # user - Lease does not exist exceptions</link>
+      </para>
+    </section>
+    <section
+      xml:id="trouble.client.scarylogs">
+      <title>Shell or client application throws lots of scary exceptions during normal
+        operation</title>
+      <para>Since 0.20.0 the default log level for <code>org.apache.hadoop.hbase.*</code>is DEBUG. </para>
+      <para> On your clients, edit <filename>$HBASE_HOME/conf/log4j.properties</filename> and change
+        this: <code>log4j.logger.org.apache.hadoop.hbase=DEBUG</code> to this:
+          <code>log4j.logger.org.apache.hadoop.hbase=INFO</code>, or even
+          <code>log4j.logger.org.apache.hadoop.hbase=WARN</code>. </para>
+    </section>
+    <section
+      xml:id="trouble.client.longpauseswithcompression">
+      <title>Long Client Pauses With Compression</title>
+      <para>This is a fairly frequent question on the Apache HBase dist-list. The scenario is that a
+        client is typically inserting a lot of data into a relatively un-optimized HBase cluster.
+        Compression can exacerbate the pauses, although it is not the source of the problem.</para>
+      <para>See <xref
+          linkend="precreate.regions" /> on the pattern for pre-creating regions and confirm that
+        the table isn't starting with a single region.</para>
+      <para>See <xref
+          linkend="perf.configurations" /> for cluster configuration, particularly
+          <code>hbase.hstore.blockingStoreFiles</code>,
+          <code>hbase.hregion.memstore.block.multiplier</code>, <code>MAX_FILESIZE</code> (region
+        size), and <code>MEMSTORE_FLUSHSIZE.</code>
+      </para>
+      <para>A slightly longer explanation of why pauses can happen is as follows: Puts are sometimes
+        blocked on the MemStores which are blocked by the flusher thread which is blocked because
+        there are too many files to compact because the compactor is given too many small files to
+        compact and has to compact the same data repeatedly. This situation can occur even with
+        minor compactions. Compounding this situation, Apache HBase doesn't compress data in memory.
+        Thus, the 64MB that lives in the MemStore could become a 6MB file after compression - which
+        results in a smaller StoreFile. The upside is that more data is packed into the same region,
+        but performance is achieved by being able to write larger files - which is why HBase waits
+        until the flushize before writing a new StoreFile. And smaller StoreFiles become targets for
+        compaction. Without compression the files are much bigger and don't need as much compaction,
+        however this is at the expense of I/O. </para>
+      <para> For additional information, see this thread on <link
+          xlink:href="http://search-hadoop.com/m/WUnLM6ojHm1/Long+client+pauses+with+compression&amp;subj=Long+client+pauses+with+compression">Long
+          client pauses with compression</link>. </para>
+    </section>
+    <section xml:id="trouble.client.security.rpc.krb">
+      <title>Secure Client Connect ([Caused by GSSException: No valid credentials provided...])</title>
+      <para>You may encounter the following error:</para>
+      <screen>Secure Client Connect ([Caused by GSSException: No valid credentials provided
+        (Mechanism level: Request is a replay (34) V PROCESS_TGS)])</screen>
+      <para> This issue is caused by bugs in the MIT Kerberos replay_cache component, <link
+          xlink:href="http://krbdev.mit.edu/rt/Ticket/Display.html?id=1201">#1201</link> and <link
+          xlink:href="http://krbdev.mit.edu/rt/Ticket/Display.html?id=5924">#5924</link>. These bugs
+        caused the old version of krb5-server to erroneously block subsequent requests sent from a
+        Principal. This caused krb5-server to block the connections sent from one Client (one HTable
+        instance with multi-threading connection instances for each regionserver); Messages, such as
+          <literal>Request is a replay (34)</literal>, are logged in the client log You can ignore
+        the messages, because HTable will retry 5 * 10 (50) times for each failed connection by
+        default. HTable will throw IOException if any connection to the regionserver fails after the
+        retries, so that the user client code for HTable instance can handle it further. </para>
+      <para> Alternatively, update krb5-server to a version which solves these issues, such as
+        krb5-server-1.10.3. See JIRA <link
+          xlink:href="https://issues.apache.org/jira/browse/HBASE-10379">HBASE-10379</link> for more
+        details. </para>
+    </section>
+    <section
+      xml:id="trouble.client.zookeeper">
+      <title>ZooKeeper Client Connection Errors</title>
+      <para>Errors like this...</para>
+      <programlisting>
+11/07/05 11:26:41 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
+ unexpected error, closing socket connection and attempting reconnect
+ java.net.ConnectException: Connection refused: no further information
+        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
+        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
+        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
+ 11/07/05 11:26:43 INFO zookeeper.ClientCnxn: Opening socket connection to
+ server localhost/127.0.0.1:2181
+ 11/07/05 11:26:44 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
+ unexpected error, closing socket connection and attempting reconnect
+ java.net.ConnectException: Connection refused: no further information
+        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
+        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
+        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
+ 11/07/05 11:26:45 INFO zookeeper.ClientCnxn: Opening socket connection to
+ server localhost/127.0.0.1:2181
+</programlisting>
+      <para>... are either due to ZooKeeper being down, or unreachable due to network issues. </para>
+      <para>The utility <xref
+          linkend="trouble.tools.builtin.zkcli" /> may help investigate ZooKeeper issues. </para>
+    </section>
+    <section
+      xml:id="trouble.client.oome.directmemory.leak">
+      <title>Client running out of memory though heap size seems to be stable (but the
+        off-heap/direct heap keeps growing)</title>
+      <para> You are likely running into the issue that is described and worked through in the mail
+        thread <link
+          xlink:href="http://search-hadoop.com/m/ubhrX8KvcH/Suspected+memory+leak&amp;subj=Re+Suspected+memory+leak">HBase,
+          mail # user - Suspected memory leak</link> and continued over in <link
+          xlink:href="http://search-hadoop.com/m/p2Agc1Zy7Va/MaxDirectMemorySize+Was%253A+Suspected+memory+leak&amp;subj=Re+FeedbackRe+Suspected+memory+leak">HBase,
+          mail # dev - FeedbackRe: Suspected memory leak</link>. A workaround is passing your
+        client-side JVM a reasonable value for <code>-XX:MaxDirectMemorySize</code>. By default, the
+          <varname>MaxDirectMemorySize</varname> is equal to your <code>-Xmx</code> max heapsize
+        setting (if <code>-Xmx</code> is set). Try seting it to something smaller (for example, one
+        user had success setting it to <code>1g</code> when they had a client-side heap of
+          <code>12g</code>). If you set it too small, it will bring on <code>FullGCs</code> so keep
+        it a bit hefty. You want to make this setting client-side only especially if you are running
+        the new experiemental server-side off-heap cache since this feature depends on being able to
+        use big direct buffers (You may have to keep separate client-side and server-side config
+        dirs). </para>
+
+    </section>
+    <section
+      xml:id="trouble.client.slowdown.admin">
+      <title>Client Slowdown When Calling Admin Methods (flush, compact, etc.)</title>
+      <para> This is a client issue fixed by <link
+          xlink:href="https://issues.apache.org/jira/browse/HBASE-5073">HBASE-5073</link> in 0.90.6.
+        There was a ZooKeeper leak in the client and the client was getting pummeled by ZooKeeper
+        events with each additional invocation of the admin API. </para>
+    </section>
+
+    <section
+      xml:id="trouble.client.security.rpc">
+      <title>Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided
+        (Mechanism level: Failed to find any Kerberos tgt)])</title>
+      <para> There can be several causes that produce this symptom. </para>
+      <para> First, check that you have a valid Kerberos ticket. One is required in order to set up
+        communication with a secure Apache HBase cluster. Examine the ticket currently in the
+        credential cache, if any, by running the klist command line utility. If no ticket is listed,
+        you must obtain a ticket by running the kinit command with either a keytab specified, or by
+        interactively entering a password for the desired principal. </para>
+      <para> Then, consult the <link
+          xlink:href="http://docs.oracle.com/javase/1.5.0/docs/guide/security/jgss/tutorials/Troubleshooting.html">Java
+          Security Guide troubleshooting section</link>. The most common problem addressed there is
+        resolved by setting javax.security.auth.useSubjectCredsOnly system property value to false. </para>
+      <para> Because of a change in the format in which MIT Kerberos writes its credentials cache,
+        there is a bug in the Oracle JDK 6 Update 26 and earlier that causes Java to be unable to
+        read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher. If
+        you have this problematic combination of components in your environment, to work around this
+        problem, first log in with kinit and then immediately refresh the credential cache with
+        kinit -R. The refresh will rewrite the credential cache without the problematic formatting. </para>
+      <para> Finally, depending on your Kerberos configuration, you may need to install the <link
+          xlink:href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jce/JCERefGuide.html">Java
+          Cryptography Extension</link>, or JCE. Insure the JCE jars are on the classpath on both
+        server and client systems. </para>
+      <para> You may also need to download the <link
+          xlink:href="http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html">unlimited
+          strength JCE policy files</link>. Uncompress and extract the downloaded file, and install
+        the policy jars into &lt;java-home&gt;/lib/security. </para>
+    </section>
+
+  </section>
+
+  <section
+    xml:id="trouble.mapreduce">
+    <title>MapReduce</title>
+    <section
+      xml:id="trouble.mapreduce.local">
+      <title>You Think You're On The Cluster, But You're Actually Local</title>
+      <para>This following stacktrace happened using <code>ImportTsv</code>, but things like this
+        can happen on any job with a mis-configuration.</para>
+      <programlisting>
+    WARN mapred.LocalJobRunner: job_local_0001
+java.lang.IllegalArgumentException: Can't read partitions file
+       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:111)
+       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
+       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
+       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:560)
+       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
+       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
+       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
+Caused by: java.io.FileNotFoundException: File _partition.lst does not exist.
+       at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:383)
+       at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
+       at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:776)
+       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1424)
+       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1419)
+       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.readPartitions(TotalOrderPartitioner.java:296)
+</programlisting>
+      <para>.. see the critical portion of the stack? It's...</para>
+      <programlisting>
+at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
+</programlisting>
+      <para>LocalJobRunner means the job is running locally, not on the cluster. </para>
+
+      <para>To solve this problem, you should run your MR job with your
+          <code>HADOOP_CLASSPATH</code> set to include the HBase dependencies. The "hbase classpath"
+        utility can be used to do this easily. For example (substitute VERSION with your HBase
+        version):</para>
+      <programlisting language="bourne">
+          HADOOP_CLASSPATH=`hbase classpath` hadoop jar $HBASE_HOME/hbase-server-VERSION.jar rowcounter usertable
+      </programlisting>
+      <para>See <link
+          xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath">
+          http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath</link>
+        for more information on HBase MapReduce jobs and classpaths. </para>
+    </section>
+    <section xml:id="trouble.hbasezerocopybytestring">
+      <title>Launching a job, you get java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString or class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString</title>
+      <para>See <link xlink:href="https://issues.apache.org/jira/browse/HBASE-10304">HBASE-10304 Running an hbase job jar: IllegalAccessError: class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString</link> and <link xlink:href="https://issues.apache.org/jira/browse/HBASE-11118">HBASE-11118 non environment variable solution for "IllegalAccessError: class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString"</link>.  The issue can also show up
+          when trying to run spark jobs.  See <link xlink:href="https://issues.apache.org/jira/browse/HBASE-10877">HBASE-10877 HBase non-retriable exception list should be expanded</link>.
+      </para>
+    </section>
+  </section>
+
+  <section
+    xml:id="trouble.namenode">
+    <title>NameNode</title>
+    <para>For more information on the NameNode, see <xref
+        linkend="arch.hdfs" />. </para>
+    <section
+      xml:id="trouble.namenode.disk">
+      <title>HDFS Utilization of Tables and Regions</title>
+      <para>To determine how much space HBase is using on HDFS use the <code>hadoop</code> shell
+        commands from the NameNode. For example... </para>
+      <para><programlisting language="bourne">hadoop fs -dus /hbase/</programlisting> ...returns the summarized disk
+        utilization for all HBase objects. </para>
+      <para><programlisting language="bourne">hadoop fs -dus /hbase/myTable</programlisting> ...returns the summarized
+        disk utilization for the HBase table 'myTable'. </para>
+      <para><programlisting language="bourne">hadoop fs -du /hbase/myTable</programlisting> ...returns a list of the
+        regions under the HBase table 'myTable' and their disk utilization. </para>
+      <para>For more information on HDFS shell commands, see the <link
+          xlink:href="http://hadoop.apache.org/common/docs/current/file_system_shell.html">HDFS
+          FileSystem Shell documentation</link>. </para>
+    </section>
+    <section
+      xml:id="trouble.namenode.hbase.objects">
+      <title>Browsing HDFS for HBase Objects</title>
+      <para>Sometimes it will be necessary to explore the HBase objects that exist on HDFS. These
+        objects could include the WALs (Write Ahead Logs), tables, regions, StoreFiles, etc. The
+        easiest way to do this is with the NameNode web application that runs on port 50070. The
+        NameNode web application will provide links to the all the DataNodes in the cluster so that
+        they can be browsed seamlessly. </para>
+      <para>The HDFS directory structure of HBase tables in the cluster is...
+        <programlisting>
+<filename>/hbase</filename>
+     <filename>/&lt;Table&gt;</filename>             (Tables in the cluster)
+          <filename>/&lt;Region&gt;</filename>           (Regions for the table)
+               <filename>/&lt;ColumnFamily&gt;</filename>      (ColumnFamilies for the Region for the table)
+                    <filename>/&lt;StoreFile&gt;</filename>        (StoreFiles for the ColumnFamily for the Regions for the table)
+            </programlisting>
+      </para>
+      <para>The HDFS directory structure of HBase WAL is..
+        <programlisting>
+<filename>/hbase</filename>
+     <filename>/.logs</filename>
+          <filename>/&lt;RegionServer&gt;</filename>    (RegionServers)
+               <filename>/&lt;WAL&gt;</filename>           (WAL files for the RegionServer)
+            </programlisting>
+      </para>
+      <para>See the <link
+          xlink:href="http://hadoop.apache.org/common/docs/current/hdfs_user_guide.html">HDFS User
+          Guide</link> for other non-shell diagnostic utilities like <code>fsck</code>. </para>
+      <section
+        xml:id="trouble.namenode.0size.hlogs">
+        <title>Zero size WALs with data in them</title>
+        <para>Problem: when getting a listing of all the files in a region server's .logs directory,
+          one file has a size of 0 but it contains data.</para>
+        <para>Answer: It's an HDFS quirk. A file that's currently being written to will appear to
+          have a size of 0 but once it's closed it will show its true size</para>
+      </section>
+      <section
+        xml:id="trouble.namenode.uncompaction">
+        <title>Use Cases</title>
+        <para>Two common use-cases for querying HDFS for HBase objects is research the degree of
+          uncompaction of a table. If there are a large number of StoreFiles for each ColumnFamily
+          it could indicate the need for a major compaction. Additionally, after a major compaction
+          if the resulting StoreFile is "small" it could indicate the need for a reduction of
+          ColumnFamilies for the table. </para>
+      </section>
+
+    </section>
+  </section>
+
+  <section
+    xml:id="trouble.network">
+    <title>Network</title>
+    <section
+      xml:id="trouble.network.spikes">
+      <title>Network Spikes</title>
+      <para>If you are seeing periodic network spikes you might want to check the
+          <code>compactionQueues</code> to see if major compactions are happening. </para>
+      <para>See <xref
+          linkend="managed.compactions" /> for more information on managing compactions. </para>
+    </section>
+    <section
+      xml:id="trouble.network.loopback">
+      <title>Loopback IP</title>
+      <para>HBase expects the loopback IP Address to be 127.0.0.1. See the Getting Started section
+        on <xref
+          linkend="loopback.ip" />. </para>
+    </section>
+    <section
+      xml:id="trouble.network.ints">
+      <title>Network Interfaces</title>
+      <para>Are all the network interfaces functioning correctly? Are you sure? See the
+        Troubleshooting Case Study in <xref
+          linkend="trouble.casestudy" />. </para>
+    </section>
+
+  </section>
+
+  <section
+    xml:id="trouble.rs">
+    <title>RegionServer</title>
+    <para>For more information on the RegionServers, see <xref
+        linkend="regionserver.arch" />. </para>
+    <section
+      xml:id="trouble.rs.startup">
+      <title>Startup Errors</title>
+      <section
+        xml:id="trouble.rs.startup.master-no-region">
+        <title>Master Starts, But RegionServers Do Not</title>
+        <para>The Master believes the RegionServers have the IP of 127.0.0.1 - which is localhost
+          and resolves to the master's own localhost. </para>
+        <para>The RegionServers are erroneously informing the Master that their IP addresses are
+          127.0.0.1. </para>
+        <para>Modify <filename>/etc/hosts</filename> on the region servers, from...</para>
+        <programlisting>
+# Do not remove the following line, or various programs
+# that require network functionality will fail.
+127.0.0.1               fully.qualified.regionservername regionservername  localhost.localdomain localhost
+::1             localhost6.localdomain6 localhost6
+            </programlisting>
+        <para>... to (removing the master node's name from localhost)...</para>
+        <programlisting>
+# Do not remove the following line, or various programs
+# that require network functionality will fail.
+127.0.0.1               localhost.localdomain localhost
+::1             localhost6.localdomain6 localhost6
+            </programlisting>
+      </section>
+
+      <section
+        xml:id="trouble.rs.startup.compression">
+        <title>Compression Link Errors</title>
+        <para> Since compression algorithms such as LZO need to be installed and configured on each
+          cluster this is a frequent source of startup error. If you see messages like
+          this...</para>
+        <programlisting>
+11/02/20 01:32:15 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
+java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
+        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1734)
+        at java.lang.Runtime.loadLibrary0(Runtime.java:823)
+        at java.lang.System.loadLibrary(System.java:1028)
+            </programlisting>
+        <para>.. then there is a path issue with the compression libraries. See the Configuration
+          section on <link
+            linkend="lzo.compression">LZO compression configuration</link>. </para>
+      </section>
+    </section>
+    <section
+      xml:id="trouble.rs.runtime">
+      <title>Runtime Errors</title>
+
+      <section
+        xml:id="trouble.rs.runtime.hang">
+        <title>RegionServer Hanging</title>
+        <para> Are you running an old JVM (&lt; 1.6.0_u21?)? When you look at a thread dump, does it
+          look like threads are BLOCKED but no one holds the lock all are blocked on? See <link
+            xlink:href="https://issues.apache.org/jira/browse/HBASE-3622">HBASE 3622 Deadlock in
+            HBaseServer (JVM bug?)</link>. Adding <code>-XX:+UseMembar</code> to the HBase
+            <varname>HBASE_OPTS</varname> in <filename>conf/hbase-env.sh</filename> may fix it.
+        </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.filehandles">
+        <title>java.io.IOException...(Too many open files)</title>
+        <para> If you see log messages like this...</para>
+        <programlisting>
+2010-09-13 01:24:17,336 WARN org.apache.hadoop.hdfs.server.datanode.DataNode:
+Disk-related IOException in BlockReceiver constructor. Cause is java.io.IOException: Too many open files
+        at java.io.UnixFileSystem.createFileExclusively(Native Method)
+        at java.io.File.createNewFile(File.java:883)
+</programlisting>
+        <para>... see the Getting Started section on <link
+            linkend="ulimit">ulimit and nproc configuration</link>. </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.xceivers">
+        <title>xceiverCount 258 exceeds the limit of concurrent xcievers 256</title>
+        <para> This typically shows up in the DataNode logs. </para>
+        <para> See the Getting Started section on <link
+            linkend="dfs.datanode.max.transfer.threads">xceivers configuration</link>. </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.oom-nt">
+        <title>System instability, and the presence of "java.lang.OutOfMemoryError: unable to create
+          new native thread in exceptions" HDFS DataNode logs or that of any system daemon</title>
+        <para> See the Getting Started section on <link
+            linkend="ulimit">ulimit and nproc configuration</link>. The default on recent Linux
+          distributions is 1024 - which is far too low for HBase. </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.gc">
+        <title>DFS instability and/or RegionServer lease timeouts</title>
+        <para> If you see warning messages like this...</para>
+        <programlisting>
+2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 10000
+2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 15000
+2009-02-24 10:01:36,472 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for xxx milliseconds - retrying
+           </programlisting>
+        <para>... or see full GC compactions then you may be experiencing full GC's. </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.nolivenodes">
+        <title>"No live nodes contain current block" and/or YouAreDeadException</title>
+        <para> These errors can happen either when running out of OS file handles or in periods of
+          severe network problems where the nodes are unreachable. </para>
+        <para> See the Getting Started section on <link
+            linkend="ulimit">ulimit and nproc configuration</link> and check your network. </para>
+      </section>
+      <section
+        xml:id="trouble.rs.runtime.zkexpired">
+        <title>ZooKeeper SessionExpired events</title>
+        <para>Master or RegionServers shutting down with messages like those in the logs: </para>
+        <programlisting>
+WARN org.apache.zookeeper.ClientCnxn: Exception
+closing session 0x278bd16a96000f to sun.nio.ch.SelectionKeyImpl@355811ec
+java.io.IOException: TIMED OUT
+       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
+WARN org.apache.hadoop.hbase.util.Sleeper: We slept 79410ms, ten times longer than scheduled: 5000
+INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server hostname/IP:PORT
+INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/IP:PORT remote=hostname/IP:PORT]
+INFO org.apache.zookeeper.ClientCnxn: Server connection successful
+WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x278bd16a96000d to sun.nio.ch.SelectionKeyImpl@3544d65e
+java.io.IOException: Session Expired
+       at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
+       at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
+       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
+ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired
+           </programlisting>
+        <para> The JVM is doing a long running garbage collecting which is pausing every threads
+          (aka "stop the world"). Since the RegionServer's local ZooKeeper client cannot send
+          heartbeats, the session times out. By design, we shut down any node that isn't able to
+          contact the ZooKeeper ensemble after getting a timeout so that it stops serving data that
+          may already be assigned elsewhere. </para>
+
+        <itemizedlist>
+          <listitem>
+            <para>Make sure you give plenty of RAM (in <filename>hbase-env.sh</filename>), the
+              default of 1GB won't be able to sustain long running imports.</para>
+          </listitem>
+          <listitem>
+            <para>Make sure you don't swap, the JVM never behaves well under swapping.</para>
+          </listitem>
+          <listitem>
+            <para>Make sure you are not CPU starving the RegionServer thread. For example, if you
+              are running a MapReduce job using 6 CPU-intensive tasks on a machine with 4 cores, you
+              are probably starving the RegionServer enough to create longer garbage collection
+              pauses.</para>
+          </listitem>
+          <listitem>
+            <para>Increase the ZooKeeper session timeout</para>
+          </listitem>
+        </itemizedlist>
+        <para>If you wish to increase the session timeout, add the following to your
+            <filename>hbase-site.xml</filename> to increase the timeout from the default of 60
+          seconds to 120 seconds. </para>
+        <programlisting language="xml">
+<![CDATA[<property>
+    <name>zookeeper.session.timeout</name>
+    <value>1200000</value>
+</property>
+<property>
+    <name>hbase.zookeeper.property.tickTime</name>
+    <value>6000</value>
+</property>]]>
+            </programlisting>
+           
+           <para>
+           Be aware that setting a higher timeout means that the regions served by a failed RegionServer will take at least
+           that amount of time to be transfered to another RegionServer. For a production system serving live requests, we would instead
+           recommend setting it lower than 1 minute and over-provision your cluster in order the lower the memory load on each machines (hence having
+           less garbage to collect per machine).
+           </para>
+           <para>
+           If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.
+           </para>
+<para>See <xref linkend="trouble.zookeeper.general"/> for other general information about ZooKeeper troubleshooting.
+</para>        </section>
+        <section xml:id="trouble.rs.runtime.notservingregion">
+           <title>NotServingRegionException</title>
+           <para>This exception is "normal" when found in the RegionServer logs at DEBUG level.  This exception is returned back to the client
+           and then the client goes back to hbase:meta to find the new location of the moved region.</para>
+           <para>However, if the NotServingRegionException is logged ERROR, then the client ran out of retries and something probably wrong.</para>
+        </section>
+        <section xml:id="trouble.rs.runtime.double_listed_regions">
+           <title>Regions listed by domain name, then IP</title>
+           <para>
+           Fix your DNS.  In versions of Apache HBase before 0.92.x, reverse DNS needs to give same answer
+           as forward lookup. See <link xlink:href="https://issues.apache.org/jira/browse/HBASE-3431">HBASE 3431
+           RegionServer is not using the name given it by the master; double entry in master listing of servers</link> for gorey details.
+          </para>
+        </section>
+        <section xml:id="brand.new.compressor">
+          <title>Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got
+            brand-new compressor' messages</title>
+                <para>We are not using the native versions of compression
+                    libraries.  See <link xlink:href="https://issues.apache.org/jira/browse/HBASE-1900">HBASE-1900 Put back native support when hadoop 0.21 is released</link>.
+                    Copy the native libs from hadoop under hbase lib dir or
+                    symlink them into place and the message should go away.
+                </para>
+        </section>
+        <section xml:id="trouble.rs.runtime.client_went_away">
+           <title>Server handler X on 60020 caught: java.nio.channels.ClosedChannelException</title>
+           <para>
+           If you see this type of message it means that the region server was trying to read/send data from/to a client but
+           it already went away. Typical causes for this are if the client was killed (you see a storm of messages like this when a MapReduce
+           job is killed or fails) or if the client receives a SocketTimeoutException. It's harmless, but you should consider digging in
+           a bit more if you aren't doing something to trigger them.
+           </para>
+        </section>
+
+      </section>
+    <section>
+      <title>Snapshot Errors Due to Reverse DNS</title>
+      <para>Several operations within HBase, including snapshots, rely on properly configured
+        reverse DNS. Some environments, such as Amazon EC2, have trouble with reverse DNS. If you
+        see errors like the following on your RegionServers, check your reverse DNS configuration:</para>
+      <screen>
+2013-05-01 00:04:56,356 DEBUG org.apache.hadoop.hbase.procedure.Subprocedure: Subprocedure 'backup1' 
+coordinator notified of 'acquire', waiting on 'reached' or 'abort' from coordinator.        
+      </screen>
+      <para>In general, the hostname reported by the RegionServer needs to be the same as the
+        hostname the Master is trying to reach. You can see a hostname mismatch by looking for the
+        following type of message in the RegionServer's logs at start-up.</para>
+      <screen>
+2013-05-01 00:03:00,614 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Master passed us hostname 
+to use. Was=myhost-1234, Now=ip-10-55-88-99.ec2.internal        
+      </screen>
+    </section>
+      <section xml:id="trouble.rs.shutdown">
+        <title>Shutdown Errors</title>
+  <para />
+      </section>
+
+    </section>
+
+    <section xml:id="trouble.master">
+      <title>Master</title>
+       <para>For more information on the Master, see <xref linkend="master"/>.
+       </para>
+      <section xml:id="trouble.master.startup">
+        <title>Startup Errors</title>
+          <section xml:id="trouble.master.startup.migration">
+             <title>Master says that you need to run the hbase migrations script</title>
+             <para>Upon running that, the hbase migrations script says no files in root directory.</para>
+             <para>HBase expects the root directory to either not exist, or to have already been initialized by hbase running a previous time. If you create a new directory for HBase using Hadoop DFS, this error will occur.
+             Make sure the HBase root directory does not currently exist or has been initialized by a previous run of HBase. Sure fire solution is to just use Hadoop dfs to delete the HBase root and let HBase create and initialize the directory itself.
+             </para>
+          </section>
+          <section xml:id="trouble.master.startup.zk.buffer">
+              <title>Packet len6080218 is out of range!</title>
+              <para>If you have many regions on your cluster and you see an error
+                  like that reported above in this sections title in your logs, see
+                  <link xlink:href="https://issues.apache.org/jira/browse/HBASE-4246">HBASE-4246 Cluster with too many regions cannot withstand some master failover scenarios</link>.</para>
+          </section>
+
+      </section>
+      <section xml:id="trouble.master.shutdown">
+        <title>Shutdown Errors</title>
+        <para/>
+      </section>
+
+    </section>
+
+    <section xml:id="trouble.zookeeper">
+      <title>ZooKeeper</title>
+      <section xml:id="trouble.zookeeper.startup">
+        <title>Startup Errors</title>
+          <section xml:id="trouble.zookeeper.startup.address">
+             <title>Could not find my address: xyz in list of ZooKeeper quorum servers</title>
+             <para>A ZooKeeper server wasn't able to start, throws that error. xyz is the name of your server.</para>
+             <para>This is a name lookup problem. HBase tries to start a ZooKeeper server on some machine but that machine isn't able to find itself in the <varname>hbase.zookeeper.quorum</varname> configuration.
+             </para>
+             <para>Use the hostname presented in the error message instead of the value you used. If you have a DNS server, you can set <varname>hbase.zookeeper.dns.interface</varname> and <varname>hbase.zookeeper.dns.nameserver</varname> in <filename>hbase-site.xml</filename> to make sure it resolves to the correct FQDN.
+             </para>
+          </section>
+
+      </section>
+      <section xml:id="trouble.zookeeper.general">
+          <title>ZooKeeper, The Cluster Canary</title>
+          <para>ZooKeeper is the cluster's "canary in the mineshaft". It'll be the first to notice issues if any so making sure its happy is the short-cut to a humming cluster.
+          </para>
+          <para>
+          See the <link xlink:href="http://wiki.apache.org/hadoop/ZooKeeper/Troubleshooting">ZooKeeper Operating Environment Troubleshooting</link> page. It has suggestions and tools for checking disk and networking performance; i.e. the operating environment your ZooKeeper and HBase are running in.
+          </para>
+         <para>Additionally, the utility <xref linkend="trouble.tools.builtin.zkcli"/> may help investigate ZooKeeper issues.
+         </para>
+      </section>
+
+    </section>
+
+    <section xml:id="trouble.ec2">
+       <title>Amazon EC2</title>
+          <section xml:id="trouble.ec2.zookeeper">
+             <title>ZooKeeper does not seem to work on Amazon EC2</title>
+             <para>HBase does not start when deployed as Amazon EC2 instances.  Exceptions like the below appear in the Master and/or RegionServer logs: </para>
+             <programlisting>
+  2009-10-19 11:52:27,030 INFO org.apache.zookeeper.ClientCnxn: Attempting
+  connection to server ec2-174-129-15-236.compute-1.amazonaws.com/10.244.9.171:2181
+  2009-10-19 11:52:27,032 WARN org.apache.zookeeper.ClientCnxn: Exception
+  closing session 0x0 to sun.nio.ch.SelectionKeyImpl@656dc861
+  java.net.ConnectException: Connection refused
+             </programlisting>
+             <para>
+             Security group policy is blocking the ZooKeeper port on a public address.
+             Use the internal EC2 host names when configuring the ZooKeeper quorum peer list.
+             </para>
+          </section>
+          <section xml:id="trouble.ec2.instability">
+             <title>Instability on Amazon EC2</title>
+             <para>Questions on HBase and Amazon EC2 come up frequently on the HBase dist-list. Search for old threads using <link xlink:href="http://search-hadoop.com/">Search Hadoop</link>
+             </para>
+          </section>
+          <section xml:id="trouble.ec2.connection">
+             <title>Remote Java Connection into EC2 Cluster Not Working</title>
+             <para>
+             See Andrew's answer here, up on the user list: <link xlink:href="http://search-hadoop.com/m/sPdqNFAwyg2">Remote Java client connection into EC2 instance</link>.
+             </para>
+          </section>
+
+    </section>
+
+    <section xml:id="trouble.versions">
+       <title>HBase and Hadoop version issues</title>
+          <section xml:id="trouble.versions.205">
+             <title><code>NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</title>
+             <para>Apache HBase 0.90.x does not ship with hadoop-0.20.205.x, etc.  To make it run, you need to replace the hadoop
+             jars that Apache HBase shipped with in its <filename>lib</filename> directory with those of the Hadoop you want to
+             run HBase on.  If even after replacing Hadoop jars you get the below exception:</para>
+<programlisting>
+sv4r6s38: Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
+sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init>(DefaultMetricsSystem.java:37)
+sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit>(DefaultMetricsSystem.java:34)
+sv4r6s38:       at org.apache.hadoop.security.UgiInstrumentation.create(UgiInstrumentation.java:51)
+sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:209)
+sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
+sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:229)
+sv4r6s38:       at org.apache.hadoop.security.KerberosName.&lt;clinit>(KerberosName.java:83)
+sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:202)
+sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
+</programlisting>
+      <para>you need to copy under <filename>hbase/lib</filename>, the
+          <filename>commons-configuration-X.jar</filename> you find in your Hadoop's
+          <filename>lib</filename> directory. That should fix the above complaint. </para>
+    </section>
+
+    <section
+      xml:id="trouble.wrong.version">
+      <title>...cannot communicate with client version...</title>
+      <para>If you see something like the following in your logs <computeroutput>... 2012-09-24
+          10:20:52,168 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting
+          shutdown. org.apache.hadoop.ipc.RemoteException: Server IPC version 7 cannot communicate
+          with client version 4 ...</computeroutput> ...are you trying to talk to an Hadoop 2.0.x
+        from an HBase that has an Hadoop 1.0.x client? Use the HBase built against Hadoop 2.0 or
+        rebuild your HBase passing the <command>-Dhadoop.profile=2.0</command> attribute to Maven
+        (See <xref
+          linkend="maven.build.hadoop" /> for more). </para>
+
+    </section>
+    </section>
+  <section>
+    <title>IPC Configuration Conflicts with Hadoop</title>
+    <para>If the Hadoop configuration is loaded after the HBase configuration, and you have
+      configured custom IPC settings in both HBase and Hadoop, the Hadoop values may overwrite the
+      HBase values. There is normally no need to change these settings for HBase, so this problem is
+      an edge case. However, <link
+        xlink:href="https://issues.apache.org/jira/browse/HBASE-11492">HBASE-11492</link> renames
+      these settings for HBase to remove the chance of a conflict. Each of the setting names have
+      been prefixed with <literal>hbase.</literal>, as shown in the following table. No action is
+      required related to these changes unless you are already experiencing a conflict.</para>
+    <para>These changes were backported to HBase 0.98.x and apply to all newer versions.</para>
+    <informaltable>
+      <tgroup
+        cols="2">
+        <thead>
+          <row>
+            <entry>Pre-0.98.x</entry>
+            <entry>0.98-x And Newer</entry>
+          </row>
+        </thead>
+        <tbody>
+          <row>
+            <entry><para><code>ipc.server.listen.queue.size</code></para></entry>
+            <entry><para><code>hbase.ipc.server.listen.queue.size</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.max.callqueue.size</code></para></entry>
+            <entry><para><code>hbase.ipc.server.max.callqueue.size</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.callqueue.handler.factor</code></para></entry>
+            <entry><para><code>hbase.ipc.server.callqueue.handler.factor</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.callqueue.read.share</code></para></entry>
+            <entry><para><code>hbase.ipc.server.callqueue.read.share</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.callqueue.type</code></para></entry>
+            <entry><para><code>hbase.ipc.server.callqueue.type</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.queue.max.call.delay</code></para></entry>
+            <entry><para><code>hbase.ipc.server.queue.max.call.delay</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.max.callqueue.length</code></para></entry>
+            <entry><para><code>hbase.ipc.server.max.callqueue.length</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.read.threadpool.size</code></para></entry>
+            <entry><para><code>hbase.ipc.server.read.threadpool.size</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.tcpkeepalive</code></para></entry>
+            <entry><para><code>hbase.ipc.server.tcpkeepalive</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.tcpnodelay</code></para></entry>
+            <entry><para><code>hbase.ipc.server.tcpnodelay</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.client.call.purge.timeout</code></para></entry>
+            <entry><para><code>hbase.ipc.client.call.purge.timeout</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.client.connection.maxidletime</code></para></entry>
+            <entry><para><code>hbase.ipc.client.connection.maxidletime</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.client.idlethreshold</code></para></entry>
+            <entry><para><code>hbase.ipc.client.idlethreshold</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.client.kill.max</code></para></entry>
+            <entry><para><code>hbase.ipc.client.kill.max</code></para></entry>
+          </row>
+          <row>
+            <entry><para><code>ipc.server.scan.vtime.weight </code></para></entry>
+            <entry><para><code>hbase.ipc.server.scan.vtime.weight </code></para></entry>
+          </row>
+        </tbody>
+      </tgroup>
+    </informaltable>
+  </section>
+
+  <section>
+    <title>HBase and HDFS</title>
+    <para>General configuration guidance for Apache HDFS is out of the scope of this guide. Refer to
+      the documentation available at <link
+        xlink:href="http://hadoop.apache.org/">http://hadoop.apache.org/</link> for extensive
+      information about configuring HDFS. This section deals with HDFS in terms of HBase. </para>
+    
+    <para>In most cases, HBase stores its data in Apache HDFS. This includes the HFiles containing
+      the data, as well as the write-ahead logs (WALs) which store data before it is written to the
+      HFiles and protect against RegionServer crashes. HDFS provides reliability and protection to
+      data in HBase because it is distributed. To operate with the most efficiency, HBase needs data
+    to be available locally. Therefore, it is a good practice to run an HDFS datanode on each
+    RegionServer.</para>
+    <variablelist>
+      <title>Important Information and Guidelines for HBase and HDFS</title>
+      <varlistentry>
+        <term>HBase is a client of HDFS.</term>
+        <listitem>
+          <para>HBase is an HDFS client, using the HDFS <code>DFSClient</code> class, and references
+            to this class appear in HBase logs with other HDFS client log messages.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>Configuration is necessary in multiple places.</term>
+        <listitem>
+          <para>Some HDFS configurations relating to HBase need to be done at the HDFS (server) side.
+            Others must be done within HBase (at the client side). Other settings need
+            to be set at both the server and client side.
+          </para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>Write errors which affect HBase may be logged in the HDFS logs rather than HBase logs.</term>
+        <listitem>
+          <para>When writing, HDFS pipelines communications from one datanode to another. HBase
+            communicates to both the HDFS namenode and datanode, using the HDFS client classes.
+            Communication problems between datanodes are logged in the HDFS logs, not the HBase
+            logs.</para>
+          <para>HDFS writes are always local when possible. HBase RegionServers should not
+            experience many write errors, because they write the local datanode. If the datanode
+            cannot replicate the blocks, the errors are logged in HDFS, not in the HBase
+            RegionServer logs.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>HBase communicates with HDFS using two different ports.</term>
+        <listitem>
+          <para>HBase communicates with datanodes using the <code>ipc.Client</code> interface and
+            the <code>DataNode</code> class. References to these will appear in HBase logs. Each of
+            these communication channels use a different port (50010 and 50020 by default). The
+            ports are configured in the HDFS configuration, via the
+              <code>dfs.datanode.address</code> and <code>dfs.datanode.ipc.address</code>
+            parameters.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>Errors may be logged in HBase, HDFS, or both.</term>
+        <listitem>
+          <para>When troubleshooting HDFS issues in HBase, check logs in both places for errors.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>HDFS takes a while to mark a node as dead. You can configure HDFS to avoid using stale
+          datanodes.</term>
+        <listitem>
+          <para>By default, HDFS does not mark a node as dead until it is unreachable for 630
+            seconds. In Hadoop 1.1 and Hadoop 2.x, this can be alleviated by enabling checks for
+            stale datanodes, though this check is disabled by default. You can enable the check for
+            reads and writes separately, via <code>dfs.namenode.avoid.read.stale.datanode</code> and
+              <code>dfs.namenode.avoid.write.stale.datanode settings</code>. A stale datanode is one
+            that has not been reachable for <code>dfs.namenode.stale.datanode.interval</code>
+            (default is 30 seconds). Stale datanodes are avoided, and marked as the last possible
+            target for a read or write operation. For configuration details, see the HDFS
+            documentation.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term>Settings for HDFS retries and timeouts are important to HBase.</term>
+        <listitem>
+          <para>You can configure settings for various retries and timeouts. Always refer to the
+            HDFS documentation for current recommendations and defaults. Some of the settings
+            important to HBase are listed here. Defaults are current as of Hadoop 2.3. Check the
+            Hadoop documentation for the most current values and recommendations.</para>
+          <variablelist>
+            <title>Retries</title>
+            <varlistentry>
+              <term><code>ipc.client.connect.max.retries</code> (default: 10)</term>
+              <listitem>
+                <para>The number of times a client will attempt to establish a connection with the
+                  server. This value sometimes needs to be increased. You can specify different
+                  setting for the maximum number of retries if a timeout occurs. For SASL
+                  connections, the number of retries is hard-coded at 15 and cannot be
+                  configured.</para></listitem>
+            </varlistentry>
+            <varlistentry>
+              <term><code>ipc.client.connect.max.retries.on.timeouts</code> (default: 45)</term>
+              <listitem><para>The number of times a client will attempt to establish a connection
+                with the server in the event of a timeout. If some retries are due to timeouts and
+                some are due to other reasons, this counter is added to
+                <code>ipc.client.connect.max.retries</code>, so the maximum number of retries for
+                all reasons could be the combined value.</para></listitem>
+            </varlistentry>
+            <varlistentry>
+              <term><code>dfs.client.block.write.retries</code> (default: 3)</term>
+              <listitem><para>How many times the client attempts to write to the datanode. After the
+              number of retries is reached, the client reconnects to the namenode to get a new
+              location of a datanode. You can try increasing this value.</para></listitem>
+            </varlistentry>
+          </variablelist>
+          <variablelist>
+            <title>HDFS Heartbeats</title>
+            <para>HDFS heartbeats are entirely on the HDFS side, between the namenode and datanodes.</para>
+            <varlistentry>
+              <term><code>dfs.heartbeat.interval</code> (default: 3)</term>
+              <listitem><para>The interval at which a node heartbeats.</para></listitem>
+            </varlistentry>
+            <varlistentry>
+              <term><code>dfs.namenode.heartbeat.recheck-interval</code> (default: 300000)</term>
+              <listitem>
+                <para>The interval of time between heartbeat checks. The total time before a node is
+                  marked as stale is determined by the following formula, which works out to 10
+                  minutes and 30 seconds:</para>
+                <screen> 2 * (dfs.namenode.heartbeat.recheck-interval) + 10 * 1000 * (dfs.heartbeat.interval)</screen>
+              </listitem>
+            </varlistentry>
+            <varlistentry>
+              <term><code>dfs.namenode.stale.datanode.interval</code> (default: 3000)</term>
+              <listitem>
+                <para>How long (in milliseconds) a node can go without a heartbeat before it is
+                  determined to be stale, if the other options to do with stale datanodes are
+                  configured (off by default).</para></listitem>
+            </varlistentry>
+          </variablelist>
+        </listitem>
+      </varlistentry>
+    </variablelist>
+    <variablelist>
+      <title>Connection Timeouts</title>
+      <para>Connection timeouts occur between the client (HBASE) and the HDFS datanode. They may
+        occur when establishing a connection, attempting to read, or attempting to write. The two
+        settings below are used in combination, and affect connections between the DFSClient and the
+        datanode, the ipc.cClient and the datanode, and communication between two datanodes. </para>
+      <varlistentry>
+        <term><code>dfs.client.socket-timeout</code> (default: 60000)</term>
+        <listitem>
+          <para>The amount of time before a client connection times out when establishing a
+            connection or reading. The value is expressed in milliseconds, so the default is 60
+            seconds.</para>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term><code>dfs.datanode.socket.write.timeout</code> (default: 480000)</term>
+        <listitem>
+          <para>The amount of time before a write operation times out. The default is 8
+            minutes, expressed as milliseconds.</para>
+        </listitem>
+      </varlistentry>
+    </variablelist>
+    <variablelist>
+      <title>Typical Error Logs</title>
+      <para>The following types of errors are often seen in the logs.</para>
+      <varlistentry>
+        <term><code>INFO HDFS.DFSClient: Failed to connect to /xxx50010, add to deadNodes and
+            continue java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel
+            to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending
+            remote=/region-server-1:50010]</code></term>
+        <listitem>
+          <para>All datanodes for a block are dead, and recovery is not possible. Here is the
+            sequence of events that leads to this error:</para>
+          <itemizedlist>
+            <listitem>
+              <para>The client attempts to connect to a dead datanode.</para>
+            </listitem>
+            <listitem>
+              <para>The connection fails, so the client moves down the list of datanodes and tries
+                the next one. It also fails.</para>
+            </listitem>
+            <listitem>
+              <para>When the client exhausts its entire list, it sleeps for 3 seconds and requests a
+              new list. It is very likely to receive the exact same list as before, in which case
+              the error occurs again.</para>
+            </listitem>
+          </itemizedlist>
+        </listitem>
+      </varlistentry>
+      <varlistentry>
+        <term><code>INFO org.apache.hadoop.HDFS.DFSClient: Exception in createBlockOutputStream
+            java.net.SocketTimeoutException: 69000 millis timeout while waiting for channel to be
+            ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/
+            xxx:50010]</code></term>
+        <listitem>
+          <para>This type of error indicates a write issue. In this case, the master wants to split
+            the log. It does not have a local datanode so it tries to connect to a remote datanode,
+            but the datanode is dead.</para>
+          <para>In this situation, there will be three retries (by default). If all retries fail, a
+            message like the following is logged:</para>
+          <screen>
+WARN HDFS.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block
+          </screen>
+          <para>If the operation was an attempt to split the log, the following type of message may
+            also appear:</para>
+          <screen>
+FATAL wal.WALSplitter: WriterThread-xxx Got while writing log entry to log            
+          </screen>
+        </listitem>
+      </varlistentry>
+    </variablelist>
+  </section>
+
+  <section
+    xml:id="trouble.tests">
+    <title>Running unit or integration tests</title>
+    <section
+      xml:id="trouble.HDFS-2556">
+      <title>Runtime exceptions from MiniDFSCluster when running tests</title>
+      <para>If you see something like the following</para>
+      <programlisting>...
+java.lang.NullPointerException: null
+at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes
+at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;
+at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
+at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster
+at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
+...</programlisting>
+      <para> or</para>
+      <programlisting>...
+java.io.IOException: Shutting down
+at org.apache.hadoop.hbase.MiniHBaseCluster.init
+at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
+at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
+at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster
+at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
+...</programlisting>
+      <para>... then try issuing the command <command>umask 022</command> before launching tests.
+        This is a workaround for <link
+          xlink:href="https://issues.apache.org/jira/browse/HDFS-2556">HDFS-2556</link>
+      </para>
+    </section>
+
+  </section>
+
+  <section
+    xml:id="trouble.casestudy">
+    <title>Case Studies</title>
+    <para>For Performance and Troubleshooting Case Studies, see <xref
+        linkend="casestudies" />. </para>
+  </section>
+
+  <section
+    xml:id="trouble.crypto">
+    <title>Cryptographic Features</title>
+    <section
+      xml:id="trouble.crypto.HBASE-10132">
+      <title>sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD</title>
+      <para>This problem manifests as exceptions ultimately caused by:</para>
+      <programlisting>
+Caused by: sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD
+	at sun.security.pkcs11.wrapper.PKCS11.C_DecryptUpdate(Native Method)
+	at sun.security.pkcs11.P11Cipher.implDoFinal(P11Cipher.java:795)
+</programlisting>
+      <para> This problem appears to affect some versions of OpenJDK 7 shipped by some Linux
+        vendors. NSS is configured as the default provider. If the host has an x86_64 architecture,
+        depending on if the vendor packages contain the defect, the NSS provider will not function
+        correctly. </para>
+      <para> To work around this problem, find the JRE home directory and edit the file
+          <filename>lib/security/java.security</filename>. Edit the file to comment out the line: </para>
+      <programlisting>
+security.provider.1=sun.security.pkcs11.SunPKCS11 ${java.home}/lib/security/nss.cfg
+</programlisting>
+      <para> Then renumber the remaining providers accordingly. </para>
+    </section>
+  </section>
+  
+  <section>
+    <title>Operating System Specific Issues</title>
+    <section>
+      <title>Page Allocation Failure</title>
+      <note><para>This issue is known to affect CentOS 6.2 and possibly CentOS 6.5. It may also affect
+        some versions of Red Hat Enterprise Linux, according to <link
+          xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=770545" />.</para></note>
+      <para>Some users have reported seeing the following error:</para>
+      <screen>kernel: java: page allocation failure. order:4, mode:0x20</screen>
+      <para>Raising the value of <code>min_free_kbytes</code> was reported to fix this problem. This
+      parameter is set to a percentage of the amount of RAM on your system, and is described in more
+      detail at <link
+        xlink:href="http://www.centos.org/docs/5/html/5.1/Deployment_Guide/s3-proc-sys-vm.html" />. </para>
+      <para>To find the current value on your system, run the following command:</para>
+      <screen language="bourne">[user@host]# <userinput>cat /proc/sys/vm/min_free_kbytes</userinput></screen>
+      <para>Next, raise the value. Try doubling, then quadrupling the value. Note that setting the
+        value too low or too high could have detrimental effects on your system. Consult your
+        operating system vendor for specific recommendations.</para>
+      <para>Use the following command to modify the value of <code>min_free_kbytes</code>,
+        substituting <replaceable>&lt;value&gt;</replaceable> with your intended value:</para>
+      <screen language="bourne">[user@host]# <userinput>echo &lt;value&gt; > /proc/sys/vm/min_free_kbytes</userinput></screen>
+    </section>
+  </section>
+  <section>
+    <title>JDK Issues</title>
+  <section>
+    <title>NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet</title>
+<para>
+If you see this in your logs:
+    <programlisting>Caused by: java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
+  at org.apache.hadoop.hbase.master.ServerManager.findServerWithSameHostnamePortWithLock(ServerManager.java:393)
+  at org.apache.hadoop.hbase.master.ServerManager.checkAndRecordNewServer(ServerManager.java:307)
+  at org.apache.hadoop.hbase.master.ServerManager.regionServerStartup(ServerManager.java:244)
+  at org.apache.hadoop.hbase.master.MasterRpcServices.regionServerStartup(MasterRpcServices.java:304)
+  at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:7910)
+  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2020)
+  ... 4 more</programlisting>
+then check if you compiled with jdk8 and tried to run it on jdk7.  If so, this won't work.
+Run on jdk8 or recompile with jdk7.  See <link xlink:href="https://issues.apache.org/jira/browse/HBASE-10607">HBASE-10607 [JDK8] NoSuchMethodError involving ConcurrentHashMap.keySet if running on JRE 7</link>.
+</para>
+  </section>
+  </section>
+
+</chapter>
-- 
1.7.0.4

